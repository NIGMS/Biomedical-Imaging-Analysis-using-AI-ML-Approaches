{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Augmentations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Overview\n",
    "This notebook provides a comprehensive guide on how to use data augmentation to improve the performance of deep learning models, with practical examples and a detailed workflow for training and evaluating models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "+ ``Understand Data Augmentation``:\n",
    "    - Learn why data augmentation is important for improving model generalization.\n",
    "+ ``Explore Image Augmentation Techniques``:\n",
    "    - Familiarize with transformations like rotation, translation, cropping, and color jitter.\n",
    "+ ``Hands-on Application``:\n",
    "\n",
    "    - Apply various augmentation techniques using torchvision.transforms.\n",
    "+ ``Work with Medical Imaging Datasets``:\n",
    "    - Use the BreastMNIST dataset for binary classification tasks.\n",
    "+ ``Train and Evaluate Models``:\n",
    "    - Train a ResNet-18 model on plain, augmented, and mixed datasets.\n",
    "    - Compare model performance across different training strategies.\n",
    "+ ``Use Key Python Libraries``:\n",
    "    - Gain proficiency in PyTorch, torchvision, medmnist, tqdm, and torchshow.\n",
    "+ ``Analyze Results``:\n",
    "    - Evaluate and interpret model performance using appropriate metrics.\n",
    "+ ``Best Practices``:\n",
    "    - Implement structured workflows and document work for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "**Data**\n",
    "\n",
    "The dataset we are using is a subset of the MedMNIST dataset called BreastMNIST. It consists of 780 ultrasound images at 28x28 resolution. It has two classes for binary classification, (normal + benign) are positive and (malignant) is negative. [[1,2]](#1and2)\n",
    "\n",
    "**Libraries**\n",
    "\n",
    "* ``pytorch`` and ``torchvision``: these libraries focus on designing machine learning models.\n",
    "* ``medmnist``: this library is specifically designed for reading and processing the MedMNIST dataset. It includes functions for data preparation and formatting.\n",
    "* ``tqdm``: a library used to display the progress of code loops.\n",
    "* ``torchshow``: a library used for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "In previous submodule we looked at classification as a whole. Now we will look at a method of improving the datasets which will consequently improve classification accuracy.\n",
    "\n",
    "### Why augment the data\n",
    "---\n",
    "In deep learning, the objective is to train a model using training data in such a way that the model generalizes well to unseen (test) data. This can be difficult if our training set is small and/or doesn't have much variation as the data in the training set may not truly represent the underlying data distribution. To understand this, consider how you would study for a math exam. Would you keep working the same example of a problem(s) over and over? Or would you work many different examples of the problem(s)? Which method would result in better performance on the exam? The answer would be to work a variety of examples to build a better intuition on how the problem can be solved. This same concept applies to the process of deep learning. Unfortunately, in some cases a larger dataset is not always available. So what do we do when we aren't able to acquire more appropriate examples for the training data? Well, this is where data augmentation comes in. We can artificially expand our training set by applying augmentations to the existing data to increase its variation in some aspects.\n",
    "\n",
    "![Figure 1: aug](aug0.png)\n",
    "\n",
    "### Image Augmentation\n",
    "---\n",
    "We can create an augmented image by taking the original image and applying some transformation to it. Some common transformations are rotation, translation, distortion, and cropping. The augmentations you would select are dataset and problem dependent as you wouldn't want to perform an irrelevant augmentation. For example, if your dataset consisted of brain scans and you are trying to identify if an abnormality exists in the left or right hemisphere, you wouldn't want to perform a horizontal flip because now the label corresponds to the wrong hemisphere. In this submodule we will be using [BreastMNIST](https://medmnist.com/) dataset.\n",
    "\n",
    "### Notebook workflow:\n",
    "---\n",
    "- <a href=\"#0\">Examples of augmentations</a></br>\n",
    "- <a href=\"#1\">Multiple transformations at once</a></br>\n",
    "- <a href=\"#2\">Apply the augmentation sequence and retreve plain and augmented datasets</a></br>\n",
    "- <a href=\"#A\">Training ``ResNet`` from scratch on plain dataset (no augmentation used)</a></br>\n",
    "    1. <a href=\"#A1\">Create ResNet-18 model to be learned from scratch.</a></br>\n",
    "    2. <a href=\"#A2\">Train on plain training dataset.</a></br>\n",
    "    3. <a href=\"#A3\">Evaluate on testing dataset.</a></br>\n",
    "- <a href=\"#B\">Training ``ResNet`` from scratch on augmented dataset (without original dataset)</a></br>\n",
    "    1. <a href=\"#B1\">Create ResNet-18 model to be learned from scratch.</a></br>\n",
    "    2. <a href=\"#B2\">Train on augmented training dataset.</a></br>\n",
    "    3. <a href=\"#B3\">Evaluate on testing dataset.</a></br>\n",
    "- <a href=\"#C\">Training ``ResNet`` from scratch on a random mix of both plain and augmented datasets</a></br>\n",
    "    1. <a href=\"#C1\">Create ResNet-18 model to be learned from scratch.</a></br>\n",
    "    2. <a href=\"#C2\">Train on mixed training dataset.</a></br>\n",
    "    3. <a href=\"#C3\">Evaluate on testing dataset.</a></br>\n",
    "- <a href=\"#3\">Conclusion</a></br>\n",
    "- <a href=\"#4\">References</a></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm torch git+https://github.com/xwying/torchshow.git@master\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as tf\n",
    "import torch.utils.data as data\n",
    "import torchshow as ts\n",
    "import medmnist\n",
    "from medmnist import INFO, Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PrepareDataset import Augment_Data, Get_DataSet_Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Loops import train_loop,test_loop,aug_train_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model import Create_Model_Optimizer_Criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"0\">Augmentation examples</a> \n",
    "\n",
    "We will use the Augment_Data function defined in PrepareDataset.py to augment our dataset. We can then define a transformation with **torchvision transforms**'s compose function as follows:\n",
    "\n",
    "``General Note``: uncomment the \"transforms.Normalize(mean=[.5], std=[.5])\" if you are using colored images otherwise it is not necessary for grayscale image and could give a warning.\n",
    "### No Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Augment_Data(data_flag = 'chestmnist', download = True, batch_size = 4, data_transform =None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Horizontal Flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "augmentation_transform_1 = transforms.Compose([transforms.ToTensor(), \n",
    "                                               #transforms.Normalize(mean=[.5], std=[.5]), \n",
    "                                               lambda x: tf.hflip(x)]\n",
    "                                             )\n",
    "A = Augment_Data(data_flag = 'chestmnist', download = True, batch_size = 4, data_transform =augmentation_transform_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Horizontal Flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_transform_2 = transforms.Compose([transforms.ToTensor(), \n",
    "                                               #transforms.Normalize(mean=[.5], std=[.5]), \n",
    "                                               transforms.RandomHorizontalFlip()]\n",
    "                                             )\n",
    "A = Augment_Data(data_flag = 'chestmnist', download = True, batch_size = 4, data_transform =augmentation_transform_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Brightness, Contrast, Saturation, and Hue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random brightness contrast saturation hue\n",
    "augmentation_transform_3 = transforms.Compose([transforms.ToTensor(), \n",
    "                                               #transforms.Normalize(mean=[.5], std=[.5]), \n",
    "                                               transforms.ColorJitter(brightness=(0,1), contrast=(0,1), saturation=(0,1), hue=(-0.5,0.5))]\n",
    "                                             )\n",
    "A = Augment_Data(data_flag = 'chestmnist', download = True, batch_size = 4, data_transform =augmentation_transform_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_transform_4 = transforms.Compose([transforms.ToTensor(), \n",
    "                                               #transforms.Normalize(mean=[.5], std=[.5]), \n",
    "                                               transforms.RandomRotation(degrees=(-180,180))]\n",
    "                                             )\n",
    "A = Augment_Data(data_flag = 'chestmnist', download = True, batch_size = 4, data_transform =augmentation_transform_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Resize Crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_transform_5 = transforms.Compose([transforms.ToTensor(), \n",
    "                                               #transforms.Normalize(mean=[.5], std=[.5]), \n",
    "                                               transforms.transforms.RandomResizedCrop(224)]\n",
    "                                             ) #nearest pixel interpolation  \n",
    "A = Augment_Data(data_flag = 'chestmnist', download = True, batch_size = 4, data_transform =augmentation_transform_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Knowledge Check</b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jupyterquiz==2.0.7 --quiet\n",
    "from jupyterquiz import display_quiz\n",
    "display_quiz('../quiz_files/submodule_02/kc1.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"1\">Multiple transformations at once</a> \n",
    "A very common sequence of transformations is applying normalization with mean and std of 0.5 followed by random resized crop and random horizontal flip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Multiple_Augmentation_Transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                                       #transforms.Normalize(mean=[.5], std=[.5]),\n",
    "                                                       transforms.RandomResizedCrop(224),\n",
    "                                                       transforms.RandomHorizontalFlip(),]\n",
    "                                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"2\">Apply the augmentation sequence and retrieve plain and augmented datasets</a> \n",
    "We will use the Augment_Data function to prepare the data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_train_loader,aug_train_loader,test_loader,train_evaluator,test_evaluator = Augment_Data(data_flag = 'breastmnist', download = True, batch_size = 16, data_transform = Multiple_Augmentation_Transforms,train_shuffle = True)\n",
    "_, Num_Classes = Get_DataSet_Information(data_flag = 'breastmnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"A\">Training ``ResNet`` from scratch on plain dataset (no augmentation used)</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    1. <a name=\"A1\">Create ``ResNet-18`` model to be learned from scratch.</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model, Optimizer, Criterion = Create_Model_Optimizer_Criterion(n_classes = Num_Classes, feature_extract = False, use_pretrained = False, bw = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    2. <a name=\"A2\">Train on plain training dataset.</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = train_loop(Model, plain_train_loader, None, Criterion, Optimizer, train_evaluator, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    3. <a name=\"A3\">Evaluate on testing dataset.</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_metrics = test_loop(Model,test_loader,test_evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"B\">Training ``ResNet`` from scratch on augmented dataset (without original dataset)</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    1. <a name=\"B1\">Create ``ResNet-18`` model to be learned from scratch.</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model, Optimizer, Criterion = Create_Model_Optimizer_Criterion(n_classes = Num_Classes, feature_extract = False, use_pretrained = False, bw = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    2. <a name=\"B2\">Train on training dataset.</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = train_loop(Model, aug_train_loader, None, Criterion, Optimizer, train_evaluator, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    3. <a name=\"B3\">Evaluate on testing dataset.</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_metrics = test_loop(Model,test_loader,test_evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"C\">Training ``ResNet`` from scratch on a random mix of both plain and augmented datasets</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    1. <a name=\"C1\">Create ``ResNet-18`` model to be learned from scratch.</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model, Optimizer, Criterion = Create_Model_Optimizer_Criterion(n_classes = Num_Classes, feature_extract = False, use_pretrained = False,bw = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    2. <a name=\"C2\">Train on mixed training dataset.</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = aug_train_loop(Model, plain_train_loader, aug_train_loader, Criterion, Optimizer, train_evaluator, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    3. <a name=\"C3\">Evaluate on testing dataset.</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_mix_metrics = test_loop(Model,test_loader,test_evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test comparison\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "data = {}\n",
    "data['Plain Dataset']=plain_metrics[1]\n",
    "data['Augmented Dataset']=aug_metrics[1]\n",
    "data['Mixed Dataset']=random_mix_metrics[1]\n",
    "networks = list(data.keys())\n",
    "accuracies = list(data.values())\n",
    "  \n",
    "fig = plt.figure(figsize = (10, 5))\n",
    " \n",
    "# creating the bar plot\n",
    "plt.bar(networks[0], accuracies[0], color ='blue',\n",
    "        width = 0.3)\n",
    "plt.bar(networks[1], accuracies[1], color ='red',\n",
    "        width = 0.3)\n",
    "plt.bar(networks[2], accuracies[2], color ='green',\n",
    "        width = 0.3)\n",
    "plt.ylim((0,1.))\n",
    "\n",
    "plt.xlabel(\"ResNet-18 Networks\")\n",
    "plt.ylabel(\"Accuracies\")\n",
    "plt.title(\"Test Set Results\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Knowledge Check</b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz('../quiz_files/submodule_02/kc2.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We see that the model trained using mixed dataset (plain + augmented) outperforms the accuracies of using the plain dataset or the augmented dataset seperately.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We can see that the model trained using mixed dataset (plain + augmented) outperforms the accuracies of using the plain dataset or the augmented dataset seperately.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "To keep your workspaced organized remember to: \n",
    "\n",
    "1. Save your work.\n",
    "2. Shut down any notebooks and active sessions to avoid extra charges.\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
