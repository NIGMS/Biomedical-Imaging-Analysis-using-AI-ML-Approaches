{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f912b87-7197-48f7-884f-ac42acdce4ef",
   "metadata": {},
   "source": [
    "# Augmentations\n",
    "---\n",
    "\n",
    "In previous submodule we looked at classification as a whole. Now we will look at a method of improving the datasets which will consequently improve classification accuracy.\n",
    "\n",
    "### Why augment the data\n",
    "---\n",
    "In deep learning, the objective is to to train a model using training data in such a way that the model is able to generalizes well to unseen (test) data. This can be difficult if our training set is small and/or doesn't have much variation as the data in the training set may not truly represent the underlying data distribution. To understand this, consider how you would study for a math exam. Would you keep working the same example of a problem(s) over and over? Or would you work many different examples of the problem(s)? Which method would result in better performance on the exam? The answer would be to work variety of examples to build a better intuition on how the problem can be solved. This same concept applies to the process of deep learning. Unfortunately, in some cases a larger dataset is not always available. So what do we do when we aren't able to acquire more appropriate examples for the training data? Well, this is where data augmentation comes in. We can artificially expand our training set by applying augmentations to the existing data to increase its variation in some aspects.\n",
    "\n",
    "![Figure 1: aug](aug0.png)\n",
    "\n",
    "### Image Augmentation\n",
    "---\n",
    "We can create an augmented image by taking the original image and applying some transformation to it. Some common transformations are rotation, translation, distortion, and cropping. The augmentations you would select are dataset and problem dependent as you wouldn't want to perform an irrelevant augmentation. For example, if your dataset consisted of brain scans and you are trying to identify if an abnormality exists in the left or right hemisphere, you wouldn't want to perform a horizontal flip because now the label corresponds to the wrong hemisphere. In this submodule we will be using [BreastMNIST](https://medmnist.com/) dataset.\n",
    "\n",
    "### BreastMNIST\n",
    "---\n",
    "The dataset we are using is a subset of the MedMNIST dataset called BreastMNIST. It consists of 780 ultrasound images at 28x28 resolution. It has two classes for binary classification, (normal + benign) are positive and (malignant) is negative. [[1,2]](#1and2)\n",
    "\n",
    "### Notebook workflow:\n",
    "---\n",
    "- <a href=\"#0\">Examples of augmentations</a></br>\n",
    "- <a href=\"#1\">Multiple transformations at once</a></br>\n",
    "- <a href=\"#2\">Apply the augmentation sequence and retreve plain and augmented datasets</a></br>\n",
    "- <a href=\"#A\">Training ``ResNet`` from scratch on plain dataset (no augmentation used)</a></br>\n",
    "    1. <a href=\"#A1\">Create ResNet-18 model to be learned from scratch.</a></br>\n",
    "    2. <a href=\"#A2\">Train on plain training dataset.</a></br>\n",
    "    3. <a href=\"#A3\">Evaluate on testing dataset.</a></br>\n",
    "- <a href=\"#B\">Training ``ResNet`` from scratch on augmented dataset (without original dataset)</a></br>\n",
    "    1. <a href=\"#B1\">Create ResNet-18 model to be learned from scratch.</a></br>\n",
    "    2. <a href=\"#B2\">Train on augmented training dataset.</a></br>\n",
    "    3. <a href=\"#B3\">Evaluate on testing dataset.</a></br>\n",
    "- <a href=\"#C\">Training ``ResNet`` from scratch on a random mix of both plain and augmented datasets</a></br>\n",
    "    1. <a href=\"#C1\">Create ResNet-18 model to be learned from scratch.</a></br>\n",
    "    2. <a href=\"#C2\">Train on mixed training dataset.</a></br>\n",
    "    3. <a href=\"#C3\">Evaluate on testing dataset.</a></br>\n",
    "- <a href=\"#3\">Conclusion</a></br>\n",
    "- <a href=\"#4\">References</a></br>\n",
    "    \n",
    "### Main libraries\n",
    "---\n",
    "The main Python libraries that will be used in this tutorial are:\n",
    "\n",
    "* ``pytorch`` and ``torchvision``: these libraries focus on designing machine learning models.\n",
    "* ``medmnist``: this library is specifically designed for reading and processing the MedMNIST dataset. It includes functions for data preparation and formatting.\n",
    "* ``tqdm``: a library used to display the progress of code loops.\n",
    "* ``torchshow``: a library used for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4271a409-25d2-46b1-ac34-7880dbcd93fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm torchshow torch\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as tf\n",
    "import torch.utils.data as data\n",
    "import torchshow as ts\n",
    "import medmnist\n",
    "from medmnist import INFO, Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e759c9c0-6b6f-4a9c-91aa-3d0339392c27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PrepareDataset import Augment_Data, Get_DataSet_Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c4c007-1cb6-413e-b33a-f4e4d82efc59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Loops import train_loop,test_loop,aug_train_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484042ef-7a98-46ac-a053-395946840fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model import Create_Model_Optimizer_Criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648fa0c2-3b78-4c42-8d7b-ef2c1fa851de",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <a name=\"0\">Augmentation examples</a> \n",
    "---\n",
    "We will use the Augment_Data function defined in PrepareDataset.py to augment our dataset. We can then define a transformation with **torchvision transforms**'s compose function as follows:\n",
    "\n",
    "``General Note``: uncomment the \"transforms.Normalize(mean=[.5], std=[.5])\" if you are using colored images otherwise it is not necessary for grayscale image and could give a warninng\n",
    "### No Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f70cc1-c358-4a27-a8f5-029149dcbabd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "A = Augment_Data(data_flag = 'chestmnist', download = True, batch_size = 4, data_transform =None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a767ec70-f61c-4fe7-a133-42f400a548e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Horizontal Flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5e0488-bce6-4fe9-9353-a19ae5910285",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "augmentation_transform_1 = transforms.Compose([transforms.ToTensor(), \n",
    "                                               #transforms.Normalize(mean=[.5], std=[.5]), \n",
    "                                               lambda x: tf.hflip(x)]\n",
    "                                             )\n",
    "A = Augment_Data(data_flag = 'chestmnist', download = True, batch_size = 4, data_transform =augmentation_transform_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a48116b-cce8-42fa-bd0a-c911d4f75cdc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Random Horizontal Flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b172cb-baaf-45fc-8d07-dceb1dd6bc3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "augmentation_transform_2 = transforms.Compose([transforms.ToTensor(), \n",
    "                                               #transforms.Normalize(mean=[.5], std=[.5]), \n",
    "                                               transforms.RandomHorizontalFlip()]\n",
    "                                             )\n",
    "A = Augment_Data(data_flag = 'chestmnist', download = True, batch_size = 4, data_transform =augmentation_transform_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcefd2f3-76f4-43a2-9741-d16092130675",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Random Brightness, Contrast, Saturation, and Hue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fc5ca5-a088-4d5e-8cf3-d08d02449cad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#random brightness contrast saturation hue\n",
    "augmentation_transform_3 = transforms.Compose([transforms.ToTensor(), \n",
    "                                               #transforms.Normalize(mean=[.5], std=[.5]), \n",
    "                                               transforms.ColorJitter(brightness=(0,1), contrast=(0,1), saturation=(0,1), hue=(-0.5,0.5))]\n",
    "                                             )\n",
    "A = Augment_Data(data_flag = 'chestmnist', download = True, batch_size = 4, data_transform =augmentation_transform_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc122a68-0373-4594-b7de-8833163bc202",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Random Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdf8b55-64db-4ad3-becd-6c51218b4f20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "augmentation_transform_4 = transforms.Compose([transforms.ToTensor(), \n",
    "                                               #transforms.Normalize(mean=[.5], std=[.5]), \n",
    "                                               transforms.RandomRotation(degrees=(-180,180))]\n",
    "                                             )\n",
    "A = Augment_Data(data_flag = 'chestmnist', download = True, batch_size = 4, data_transform =augmentation_transform_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78b3b0b-f8a9-4211-bfd4-c857262f3a40",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Random Resize Crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522a69ff-234c-43aa-aa0e-692b45ac850a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "augmentation_transform_5 = transforms.Compose([transforms.ToTensor(), \n",
    "                                               #transforms.Normalize(mean=[.5], std=[.5]), \n",
    "                                               transforms.transforms.RandomResizedCrop(224)]\n",
    "                                             ) #nearest pixel interpolation  \n",
    "A = Augment_Data(data_flag = 'chestmnist', download = True, batch_size = 4, data_transform =augmentation_transform_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369d8247-2f84-4bad-8290-fc75c414d5eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Knowledge Check</b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779b8e30-6460-41fe-9cbf-9e30940f3951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, IFrame\n",
    "IFrame('../quiz_files/submodule_02/quiz1.html', width=800, height=430)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9936c36-70b0-488d-8663-fd2ee956089e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <a name=\"1\">Multiple transformations at once</a> \n",
    "A very common sequence of transformations is applying normalization with mean and std of 0.5 followed by random resized crop and random horizontal flip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4853d02-d85a-46a6-b85f-71cab7528825",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multiple_Augmentation_Transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                                       #transforms.Normalize(mean=[.5], std=[.5]),\n",
    "                                                       transforms.RandomResizedCrop(224),\n",
    "                                                       transforms.RandomHorizontalFlip(),]\n",
    "                                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ca9674-185a-45eb-8a0a-17ae28424e40",
   "metadata": {},
   "source": [
    "## <a name=\"2\">Apply the augmentation sequence and retrieve plain and augmented datasets</a> \n",
    "We will use the Augment_Data function to prepare the data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb5b6ef-c623-40da-ab6c-419779d1e434",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plain_train_loader,aug_train_loader,test_loader,train_evaluator,test_evaluator = Augment_Data(data_flag = 'breastmnist', download = True, batch_size = 16, data_transform = Multiple_Augmentation_Transforms,train_shuffle = True)\n",
    "_, Num_Classes = Get_DataSet_Information(data_flag = 'breastmnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f6da39-f894-48de-b713-1e8038922247",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <a name=\"A\">Training ``ResNet`` from scratch on plain dataset (no augmentation used)</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7f7387-b75d-4082-9f23-673187ed9556",
   "metadata": {},
   "source": [
    "####    1. <a name=\"A1\">Create ``ResNet-18`` model to be learned from scratch.</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482eb30d-724e-47ab-a26b-c502a6326bb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Model, Optimizer, Criterion = Create_Model_Optimizer_Criterion(n_classes = Num_Classes, feature_extract = False, use_pretrained = False, bw = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a78f15c-9e8d-496e-a527-cfb8dc678a63",
   "metadata": {
    "tags": []
   },
   "source": [
    "####    2. <a name=\"A2\">Train on plain training dataset.</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268dca98-7af9-42e2-9893-95626276c054",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = train_loop(Model, plain_train_loader, None, Criterion, Optimizer, train_evaluator, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f132e2e-d3ac-48dd-b54b-d40ee783522e",
   "metadata": {
    "tags": []
   },
   "source": [
    "####    3. <a name=\"A3\">Evaluate on testing dataset.</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91de1e4-5928-4548-a822-1ae0744776d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_metrics = test_loop(Model,test_loader,test_evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3501995-7b26-4abf-b475-c61e6356f667",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <a name=\"B\">Training ``ResNet`` from scratch on augmented dataset (without original dataset)</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbf8ec4-54aa-4d2a-b972-956be44e1541",
   "metadata": {
    "tags": []
   },
   "source": [
    "####    1. <a name=\"B1\">Create ``ResNet-18`` model to be learned from scratch.</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da65dca-50dd-4ff8-923c-e5cea0cf9896",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model, Optimizer, Criterion = Create_Model_Optimizer_Criterion(n_classes = Num_Classes, feature_extract = False, use_pretrained = False, bw = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c849037-d18f-4ed5-8e8e-fd7cd211ae51",
   "metadata": {},
   "source": [
    "####    2. <a name=\"B2\">Train on training dataset.</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905140ae-5d51-4bb0-8379-6df627aa1dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = train_loop(Model, aug_train_loader, None, Criterion, Optimizer, train_evaluator, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913f9932-57c5-4aab-bd15-1ae2e7e9725b",
   "metadata": {
    "tags": []
   },
   "source": [
    "####    3. <a name=\"B3\">Evaluate on testing dataset.</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72610f15-d12d-4316-93e7-704f291e9771",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_metrics = test_loop(Model,test_loader,test_evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8102da58-8713-43af-b5b5-a7acd912f4f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <a name=\"C\">Training ``ResNet`` from scratch on a random mix of both plain and augmented datasets</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7dafde-9e7b-4a7f-b95e-b02d078c7cec",
   "metadata": {
    "tags": []
   },
   "source": [
    "####    1. <a name=\"C1\">Create ``ResNet-18`` model to be learned from scratch.</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db45ee24-f437-40c1-b37a-1a062cbb67b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model, Optimizer, Criterion = Create_Model_Optimizer_Criterion(n_classes = Num_Classes, feature_extract = False, use_pretrained = False,bw = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb98d0-b066-4409-8fb0-6dcdab768ed1",
   "metadata": {},
   "source": [
    "####    2. <a name=\"C2\">Train on mixed training dataset.</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f42a9b-a412-44f1-b39f-1301608811b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = aug_train_loop(Model, plain_train_loader, aug_train_loader, Criterion, Optimizer, train_evaluator, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30207886-b6aa-4f51-afa2-d0f31d3a3673",
   "metadata": {
    "tags": []
   },
   "source": [
    "####    3. <a name=\"C3\">Evaluate on testing dataset.</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c70eb16-8ad4-4677-a80c-3e853add40da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_mix_metrics = test_loop(Model,test_loader,test_evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056909c2-d9f5-4edb-8b24-f8aaccc7c255",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test comparison\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "data = {}\n",
    "data['Plain Dataset']=plain_metrics[1]\n",
    "data['Augmented Dataset']=aug_metrics[1]\n",
    "data['Mixed Dataset']=random_mix_metrics[1]\n",
    "networks = list(data.keys())\n",
    "accuracies = list(data.values())\n",
    "  \n",
    "fig = plt.figure(figsize = (10, 5))\n",
    " \n",
    "# creating the bar plot\n",
    "plt.bar(networks[0], accuracies[0], color ='blue',\n",
    "        width = 0.3)\n",
    "plt.bar(networks[1], accuracies[1], color ='red',\n",
    "        width = 0.3)\n",
    "plt.bar(networks[2], accuracies[2], color ='green',\n",
    "        width = 0.3)\n",
    "plt.ylim((0,1.))\n",
    "\n",
    "plt.xlabel(\"ResNet-18 Networks\")\n",
    "plt.ylabel(\"Accuracies\")\n",
    "plt.title(\"Test Set Results\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64514a33-c060-4c4b-ad59-fe7ff8a16e3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Knowledge Check</b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe5efc8-a912-452a-bd53-f50ee4c37cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame('../quiz_files/submodule_02/quiz2.html', width=800, height=430)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed02aaaa-c925-4dd4-a10e-dd02dcd234a1",
   "metadata": {},
   "source": [
    "## <a name=\"3\">Conclusion</a> \n",
    "\n",
    "We can see that the model trained using mixed dataset (plain + augmented) outperforms the accuracies of using the plain dataset or the augmented dataset seperately.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b80989-791e-4ef2-b27b-c51b98d1b8fe",
   "metadata": {},
   "source": [
    "## <a name=\"4\">References</a>\n",
    "---\n",
    "<a id=\"1and2\"></a>\n",
    "[1]   Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, Bingbing Ni. \"MedMNIST v2: A Large-Scale Lightweight Benchmark for 2D and 3D Biomedical Image Classification\". arXiv preprint arXiv:2110.14795, 2021.\n",
    "\n",
    "[2]   Jiancheng Yang, Rui Shi, Bingbing Ni. \"MedMNIST Classification Decathlon: A Lightweight AutoML Benchmark for Medical Image Analysis\". IEEE 18th International Symposium on Biomedical Imaging (ISBI), 2021.\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m103"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
