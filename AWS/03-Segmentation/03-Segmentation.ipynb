{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Segmentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Overview\n",
    "\n",
    "In the previous submodules, we demonstrated how a deep learning neural network can bin images into distinct groups based on features within the image. However, these network architectures only allow for a _single_ classification output and lack any _spatial information_. This limitation may not be ideal for large images that contains more than one class within the image. \n",
    "\n",
    "Consider a self-driving car that is powered by artificial intelligence. Self-driving cars have a large amount of cameras, sensors, etc. that are used to inform the onboard computer about the surrounding environment. When a self-driving car acquires a snapshot of its surroundings (which happens ~30 times a second), the computer must evaluate the collected information and decide if the vehicle is going to accelerate, brake, turn, etc. One important step of this process is to identify its surrounding environment, which is difficult to do with a single output using a traditional neural network.\n",
    "\n",
    "Alternatively, when an image is taken, a trained neural network can be scanned, or _convolved_, across an entire image to eventually build up a spatial classification map of its surroundings. This type of process has previously been used in many applications, including segmenting neuronal membranes in biomedical images [<a href=\"#Reference1\" class=\"intrnllnk\">1</a>]. Overall, however, this approach is relatively time consuming, and can not be completed before the next snapshot arrives for a self-driving car.\n",
    "\n",
    "A more elegant approach is to generate a _fully convolutional_ neural network. As a brief refresher from the classification submodule: deep learning neural networks _encode_ information from the input image, typically with a _max pooling_ layer, which takes the maximum of an output map within a small region. This process in turn decreases the size of the output map by half while keeping the most important features. When this process is done multiple times, spatial information from the input image is encoded and used to predict a class from the image. However, once features from an image are encoded, they can also be _decoded_, and by performing the same number of encoding and decoding steps, a pixel-wise classification map can be produced. When this network architecture is drawn out, the shape of the encoding/decoding steps results in a \"U\" shape, resulting in the network being called a \"U-Net\" [<a href=\"#Reference2\" class=\"intrnllnk\">2</a>]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "+ ``Understand Image Segmentation``: \n",
    "    - Grasp the concept of image segmentation as a pixel-wise classification task using deep learning, contrasting it with traditional image classification. \n",
    "    - Learn about the benefits of segmentation, particularly in scenarios requiring spatial information, like self-driving cars and biomedical image analysis.\n",
    "+ ``Understand the U-Net Architecture``: \n",
    "    - Learn the structure and function of the U-Net architecture, including its encoding and decoding paths, convolutional layers, max-pooling, upscaling, and skip connections.\n",
    "    - Develop the ability to interpret and modify the U-Net architecture in code, including adding additional encoding/decoding steps.\n",
    "+ ``Apply U-Net to a Biomedical Problem``: \n",
    "    - Apply the U-Net architecture to a real-world biomedical image segmentation problem (skin layer segmentation).\n",
    "    - Understand the importance of pre-processing and preparing data for a specific task, including normalizing images and simplifying classification labels.\n",
    "+ ``Implement Data Loading and Preprocessing``: \n",
    "    - Gain practical experience loading, preprocessing, and augmenting image data using libraries like imageio, torchvision, and numpy. \n",
    "    - Learn how to create custom dataset classes (UNetDataset) and data loaders (DataLoader) in PyTorch for efficient data handling during training. \n",
    "    - Experiment with different augmentation techniques (e.g., horizontal/vertical flipping) and their impact on network robustness.\n",
    "+ ``Train a Segmentation Network``: \n",
    "    - Learn how to train a U-Net segmentation network using PyTorch, including defining a loss function (CrossEntropyLoss), an optimizer (Adam), and implementing a training loop.\n",
    "    - Understand the concepts of epochs, batch size, backpropagation, and optimization steps.\n",
    "    - Learn how to monitor training progress by visualizing training and validation accuracy and loss.\n",
    "+ ``Evaluate Segmentation Performance``:\n",
    "    - Evaluate the performance of the trained U-Net model on training, validation, and testing datasets. \n",
    "    - Understand the importance of using a separate test set for unbiased evaluation.\n",
    "    - Interpret accuracy and loss metrics to assess the network's performance.\n",
    "    - Visualize the segmented output and compare it to the ground truth masks to qualitatively assess the results.\n",
    "+ ``Address Class Imbalance``:\n",
    "    - Recognize the challenges posed by class imbalance in datasets and explore strategies to mitigate its effects on training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "**Data**\n",
    "\n",
    "The notebook uses s3fs-fuse to access data stored in an AWS S3 bucket. This means:\n",
    "+ You'll need an AWS account.\n",
    "+ You need the s3fs-fuse tool installed on your system. You can install it following the instructions here: https://github.com/s3fs-fuse/s3fs-fuse.\n",
    "+ The notebook assumes the data is in a bucket accessible to your project. The bucket path is specified as 'nigms-sandbox/nosi-uams-alml/segmentation_data_small'. You may need to adjust this path if your data is in a different location. You need the appropriate permissions to access the bucket. The notebook is trying to mount the bucket to /home/jupyter/segmentation_bucket/. Ensure this directory exists and is writable by the user running the notebook. It's also important to note that data access speeds can be affected by your internet connection and the performance of your AWS project.\n",
    "\n",
    "**Required packages**\n",
    "\n",
    "For this submodule, we will be using the following packages:\n",
    "- `PyTorch` (torch) and `torchvision` for neural network generation and training\n",
    "- `tqdm` for progress bars\n",
    "- `imageio` for data loading\n",
    "- `matplotlib` and `numpy` for data visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that after running the next code section, the following warning will appear and can be ignored:\n",
    "> /opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
    "  from .autonotebook import tqdm as notebook_tqdm\n",
    "  \n",
    "This section will also install `jupyterquiz` if it is not already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all of the necessary packages\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import imageio.v3 as iio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import os\n",
    "import os.path as osp\n",
    "from IPython.display import clear_output\n",
    "\n",
    "!pip install jupyterquiz==2.0.7\n",
    "from jupyterquiz import display_quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: Due to the size of the U-Net network, training _WILL_ take a very long time without utilizing a GPU. A notebook configured with a GPU is _HIGHLY RECOMMENDED_ for this submodule!**\n",
    "\n",
    "In preparation for this submodule, we will initialize a GPU and mount the bucket that contains the image data used in this submodule. This section of code will create a new folder named `segmentation_bucket`, and populate it will all of the image data needed for this submodule.\n",
    "\n",
    "To get all of the image data, the `s3fs-fuse` command is called, which pulls a specific folder from a _bucket_, which is a specific location on the AWS where the data are stored. This is similar to downloading a public dataset as seen in previous submodules, but is particularly useful for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\");\n",
    "\n",
    "# Prepare the folder system that will be used for mounting portions of a bucket\n",
    "for folderList in ['training','validation','testing']:\n",
    "    for subList in ['images','masks']:\n",
    "        os.system('mkdir -p /home/jupyter/segmentation_bucket/%s/%s' % (folderList, subList))\n",
    "\n",
    "# Mount subfolders of the main bucket\n",
    "rootBucket = 'my-aws-bucket/segmentation_data_small';\n",
    "saveDir = '/home/jupyter/segmentation_bucket/';\n",
    "\n",
    "for folderList in ['training','validation','testing']:\n",
    "    for subList in ['images','masks']:\n",
    "        os.system('s3fs nigms-sandbox /home/jupyter/segmentation_bucket/%s/%s -o allow_other -o use_cache=/tmp' % (folderList, subList));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a U-Net architecture\n",
    "---\n",
    "The original U-Net architecture is made up of 4 encoding and decoding blocks, with each consisting of two convolutional layers. Feature map encoding is performed with max pooling operations, and an upscaling operation is used in the decoding path. Finally, to add more context during the decoding path, the feature map from the corresponding encoding block is added to the decoding block. This architecture ultimately forms a \"U\" shape. Here is a schematic of the network architecture [<a href=\"#Reference2\" class=\"intrnllnk\">2</a>]:\n",
    "\n",
    "<p style=\"text-align: center\"> <img src=\"images/unet.png\" alt=\"Unet architecture\"/> </p>\n",
    "\n",
    "The following code block generates a U-Net architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_conv(in_channels, out_channels):\n",
    "    # This corresponds to the two convolution layers that make up each encoding/decoding block\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        self.n_class = n_class\n",
    "\n",
    "        # This is just initializing each encoding/decoding block, as well as the max pooling layer that will be used\n",
    "        self.dconv_down1 = double_conv(3, 64)\n",
    "        self.dconv_down2 = double_conv(64, 128)\n",
    "        self.dconv_down3 = double_conv(128, 256)\n",
    "        self.dconv_down4 = double_conv(256, 512)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "\n",
    "        self.dconv_up3 = double_conv(256 + 512, 256)\n",
    "        self.dconv_up2 = double_conv(128 + 256, 128)\n",
    "        self.dconv_up1 = double_conv(128 + 64, 64)\n",
    "\n",
    "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #--- Encoding Path ---#\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool(conv1)\n",
    "\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(conv2)\n",
    "\n",
    "        conv3 = self.dconv_down3(x)\n",
    "        \n",
    "        x = self.maxpool(conv3) # Encoding\n",
    "\n",
    "        x = self.dconv_down4(x)\n",
    "        \n",
    "        #--- Decoding Path ---#\n",
    "        x = nn.functional.interpolate(x, scale_factor=2, mode='nearest') #  Decoding\n",
    "     \n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "\n",
    "        x = self.dconv_up3(x)\n",
    "        x = nn.functional.interpolate(x, scale_factor=2, mode='nearest')\n",
    "        x = torch.cat([x, conv2], dim=1)\n",
    "\n",
    "        x = self.dconv_up2(x)\n",
    "        x = nn.functional.interpolate(x, scale_factor=2, mode='nearest')\n",
    "        x = torch.cat([x, conv1], dim=1)\n",
    "\n",
    "        x = self.dconv_up1(x)\n",
    "\n",
    "        out = self.conv_last(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"color:black\"> <b>EXERCISE</b> The default U-Net architecture performs encoding (and decoding) 4 times. Following the pattern of the network architecture, try adding an additional encoding and decoding steps and see how that affects the output of the network. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeing U-Net in action\n",
    "---\n",
    "To verify that the network is correctly producing the expected output, we can initialize the network with random weights and biases, apply the network to an image, and observe the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample image from the imageio package\n",
    "# In this case, an example image of immunohistochemistry (IHC) staining is used\n",
    "img = iio.imread(\"images/immunohistochemistry.png\")\n",
    "\n",
    "# Create a figure to show the original image and the network output\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "# Initial Image\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.set_title('Original Image')\n",
    "ax.axis('off')\n",
    "plot1 = plt.imshow(img)\n",
    "\n",
    "# Initialize the U-Net network model with just a single class. This will not matter since it is just to verify the network output\n",
    "model = UNet(n_class=1)\n",
    "\n",
    "# Convert the test image to the expected input type for the network\n",
    "img_torch = torch.tensor(img / 255).permute(2,0,1).unsqueeze(0).float()\n",
    "\n",
    "# Pass the input image into the network\n",
    "output = model(img_torch)\n",
    "\n",
    "# Show the output image\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.set_title('Output Image from U-Net')\n",
    "ax.axis('off')\n",
    "plot2 = plt.imshow(output.squeeze().detach().numpy(),cmap='gray',clim=(torch.min(output),torch.max(output)))\n",
    "\n",
    "# Print the height and width of the input and output image\n",
    "print(\"Shape of input image: \",np.shape(np.mean(img,axis=2)))\n",
    "print(\"Shape of output image: \",np.shape(output.squeeze().detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the actual values of the outputted image do not mean anything, we can clearly see that the output of the network is the same size as the input image. This means that we can use this network architecture to semantically segment an image.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Knowledge Check</b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jupyterquiz --quiet\n",
    "from jupyterquiz import display_quiz\n",
    "display_quiz('../quiz_files/submodule_03/kc1.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a segmentation network\n",
    "---\n",
    "Similar to previous classifcation submodules, the dataset used to train a segmentation network must have an input image as well as some type of label. However, instead of a single label for an input image, a pixel-wise labeled image must be used to determine the accuracy of the network.\n",
    "For example, here is a labeled image from the [cityscapes dataset](https://www.cityscapes-dataset.com) where the pixels are color-coded as:\n",
    "\n",
    "<div>\n",
    "    <table>\n",
    "        <thead>\n",
    "            <tr>\n",
    "                <th>Color</th>\n",
    "                <th>Label</th>\n",
    "            </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "            <tr>\n",
    "                <td> <span style=\"color:#B06BB0;\">purple</span> </td>\n",
    "                <td> road </td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td> <span style=\"color:#FF39F5;\">pink</span> </td>\n",
    "                <td> sidewalk </td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td> <span style=\"color:#FF1A15;\">red</span> </td>\n",
    "                <td> human </td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td> <span style=\"color:#2024FF;\">blue</span> </td>\n",
    "                <td> car </td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td> <span style=\"color:#EFF215;\">yellow</span> </td>\n",
    "                <td> traffic sign </td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td> <span style=\"color:#949695;\">light gray</span> </td>\n",
    "                <td> pole </td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td> <span style=\"color:#62665F;\">dark gray</span> </td>\n",
    "                <td> building </td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td> <span style=\"color:#000000;\">black</span> </td>\n",
    "                <td> clutter </td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td> <span style=\"color:#997112;\">dark yellow</span> </td>\n",
    "                <td> street sign </td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td> <span style=\"color:#76B1DB;\">light blue</span> </td>\n",
    "                <td> sky </td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td> <span style=\"color:#781C2B;\">dark red</span> </td>\n",
    "                <td> bicycle </td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td> <span style=\"color:#7AA12E;\">green</span> </td>\n",
    "                <td> vegetation </td>\n",
    "            </tr>        \n",
    "        </tbody>\n",
    "    </table>\n",
    "\n",
    "<p style=\"text-align: center\"> <img src=\"images/tuebingen00.png\" alt=\"labeled image of city\" width=\"1000\"/> </p>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application of a U-Net architecture\n",
    "---\n",
    "As previously stated, segmentation is a powerful tool, particularly for biomedical image datasets. For an example application of segmentation with a U-Net, we will be segmenting images of skin that were acquired using fluorescence microscopy. The dataset we will be using for this application has previously been published [<a href=\"#Reference3\" class=\"intrnllnk\">3</a>,<a href=\"#Reference4\" class=\"intrnllnk\">4</a>].\n",
    "\n",
    "Skin is made up of three primary layers (_epidermis, dermis, hypodermis_) each with its own structure and function. These layers aggregate to form a protective boundary for the human body.\n",
    "<p style=\"text-align: center\"> <img src=\"images/skinlayers.png\" alt=\"layers of skin\" width=\"600\" /></p>\n",
    "\n",
    "<p> For this application, <em>en face</em> images of skin autofluorescence were acquired where the sources of contrast are two molecular cofactors associated with cellular metabolism (<span style=\"color:#00FF00;\">green</span> and <span style=\"color:#0000FF;\">blue</span>, localized to cells), as well as collagen fibers found within the dermal layer (<span style=\"color:#FF0000;\">red</span>). </p>\n",
    "\n",
    "Within skin, the cells in question are mostly localized in the epidermis, and the relative amount of the two metabolic cofactors can inform us on their cellular function (i.e. oxidative phosphorylation or glucose catabolism). Here is an example of one of these images: \n",
    "<p style=\"text-align: center\"> <img src=\"images/mpmexample_annotated.png\" alt=\"en face image of skin\" width=\"400\" /> </p>\n",
    "\n",
    "Accurate segmentation of the layers of skin in the _en face_ images is important for summarizing the metabolic state that cells are in but this is difficult and tedious to do by hand. For this submodule, a U-Net architecture will be trained to semantically segment different layers and features within skin. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and viewing the dataset\n",
    "---\n",
    "Typically, in image processing workflows, a lot of future issues can be mitigated by just observing what the data look like prior to any processing. To verify that the images used for training load correctly from the bucket, and to visualize the images, we will choose a small subset of the training dataset to display. Additionally, we will also output the corresponding labeled _ground truth_ image, which is a hand-traced pixel-wise classification map for the image. The goal is to teach the network to generate its own pixel-wise map.\n",
    "\n",
    "Please note the following code block is output the following warnings:\n",
    "\n",
    "> /opt/conda/lib/python3.7/site-packages/IPython/core/events.py:89: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
    "  func(*args, **kwargs)\n",
    ">\n",
    "> /opt/conda/lib/python3.7/site-packages/IPython/core/pylabtools.py:151: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
    "  fig.canvas.print_figure(bytes_io, **kw)\n",
    "  \n",
    "These can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all of the training images\n",
    "bucket_loc = '/home/jupyter'\n",
    "filelist = os.listdir(bucket_loc + '/segmentation_bucket/training/images/')\n",
    "\n",
    "# Pick 4 images at random\n",
    "random_int = np.random.randint(low=0, high=len(filelist), size=4)\n",
    "\n",
    "# Display the images\n",
    "fig, axs = plt.subplots(2, 4, layout=\"tight\", figsize=(15, 8))\n",
    "for i in range(4):\n",
    "    axs[0,i].imshow(iio.imread(bucket_loc + '//segmentation_bucket/training/images/'+filelist[random_int[i]]))\n",
    "    axs[0,i].set_axis_off()\n",
    "\n",
    "for i in range(4):\n",
    "    axs[1,i].imshow(iio.imread(bucket_loc + '//segmentation_bucket/training/masks/'+filelist[random_int[i]]), cmap='jet',clim=(36,216))\n",
    "    axs[1,i].set_axis_off()\n",
    "\n",
    "# Set up a colormap for reference\n",
    "cax = plt.axes([1.01, 0.1, 0.05, 0.8]);\n",
    "cmap = cm.get_cmap('jet', 6) \n",
    "ticks = np.linspace(0,1,13)\n",
    "\n",
    "cbar = plt.colorbar(mappable = cm.ScalarMappable(norm=None, cmap=cmap), cax=cax);\n",
    "cbar.ax.set_yticks(ticks[1:len(ticks)-1:2]);\n",
    "cbar.ax.set_yticklabels(['epidermis','dermis','granulation','keratin','hair','background']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 6 different labels in this dataset: epidermis, dermis, granulation, keratin, hair, background. However, for the purpose of this submodule, we will be generalizing these labels, which will simplify the training process. The new labels will be the following: epidermis, dermis, hair, and background.\n",
    "\n",
    "Here are the same four images, but with the altered classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the new classes\n",
    "classes = np.array([36,72,108,144,180,216])\n",
    "actualClass = np.array([1,2,1,1,3,4])\n",
    "\n",
    "# Display the images\n",
    "fig, axs = plt.subplots(2, 4, layout=\"tight\", figsize=(15, 8))\n",
    "for i in range(4):\n",
    "    axs[0,i].imshow(iio.imread(bucket_loc + '/segmentation_bucket/training/images/'+filelist[random_int[i]]))\n",
    "    axs[0,i].set_axis_off()\n",
    "\n",
    "for i in range(4):\n",
    "    mask = iio.imread(bucket_loc + '/segmentation_bucket/training/masks/'+filelist[random_int[i]])\n",
    "    \n",
    "    for j in range(len(classes)):\n",
    "        mask[np.where(mask==classes[j])] = actualClass[j]  \n",
    "        \n",
    "    axs[1,i].imshow(mask, cmap='jet',clim=(1,4))\n",
    "    axs[1,i].set_axis_off()\n",
    "    \n",
    "# Set up a colormap for reference\n",
    "cax = plt.axes([1.01, 0.1, 0.05, 0.8])\n",
    "cmap = cm.get_cmap('jet', 4) \n",
    "ticks = np.linspace(0,1,9)\n",
    "\n",
    "cbar = plt.colorbar(mappable = cm.ScalarMappable(norm=None, cmap=cmap), cax=cax);\n",
    "cbar.ax.set_yticks(ticks[1:len(ticks)-1:2]);\n",
    "cbar.ax.set_yticklabels(['epidermis','dermis','hair','background']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dataset class\n",
    "---\n",
    "Datasets come in all different shapes, sizes, and formats, requiring the end user (us) to load and prepare the data in a meaningful way. The purpose of this `UNetDataset` class is to prepare images as they are needed during the training process. This includes converting the images to a PyTorch tensor, performing random X/Y flipping augmentations, and combining the input image and the corresponding mask so they can be quickly referenced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDataset(torch.utils.data.Dataset):\n",
    "    # Define what will be ran at initialization of the UNetDataset class\n",
    "    \n",
    "    def __init__(self, image_array, mask_array):\n",
    "    # Attach the list of images and ages to the class\n",
    "    # These are attached to the class so that they can be accessed by other methods in the class\n",
    "        self.image = image_array\n",
    "        self.mask = mask_array\n",
    "\n",
    "    # Initialize a transform that will be used later\n",
    "        self.tform = T.ToTensor()                       \n",
    "\n",
    "        # There are two required methods for a class that inherits from torch.utils.data.Dataset:\n",
    "        # __len__()\n",
    "        # __getitem__()\n",
    "\n",
    "    # Return the length of the dataset\n",
    "    def __len__(self):\n",
    "        return self.image.shape[3]\n",
    "\n",
    "    # Return a single image from the dataset, as well as the mask associated with the image\n",
    "    def __getitem__(self,idx):\n",
    "        # Return a single variable (dict) that contains both the image and the mask\n",
    "        out = {\n",
    "            'image': self.tform(self.image[:,:,:,idx]).float(),\n",
    "            'mask': self.tform(self.mask[:,:,idx]).long()\n",
    "        }\n",
    "        \n",
    "        # Random vflip and hflip\n",
    "        if np.random.rand() > 0.5:\n",
    "            out['image'] = torchvision.transforms.functional.hflip(out['image'])\n",
    "            out['mask'] = torchvision.transforms.functional.hflip(out['mask'])\n",
    "            \n",
    "        if np.random.rand() > 0.5:\n",
    "            out['image'] = torchvision.transforms.functional.vflip(out['image'])\n",
    "            out['mask'] = torchvision.transforms.functional.vflip(out['mask'])\n",
    " \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"color:black\"> <b>EXERCISE</b> In the previous submodule, augmentation was shown to improve the robustness of image classification. Try implementing other types of augmentation aside from horizontal and vertical flipping and assess how these augmentations affect the network accuracy. A list of augmentations can be found <a href=https://pytorch.org/vision/stable/transforms.html#functional-transforms>here</a>. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing datasets\n",
    "---\n",
    "Once the U-Net architecture and dataset class are ready to go, we can load (or _buffer_) all of the images into memory. After loading a particular input image, the image is first _normalized_ such that the intensity values range between 0 and 1, and the annotated masks are adjusted to the limited number of classes as previously described.\n",
    "\n",
    "Finally, a `UNetDataset` is made for each of the three datasets (training, validation, and testing), and the dataset class is wrapped within a `DataLoader` that allows us to easily modify parameters regarding training such as shuffling and batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all of the training images\n",
    "filelist = os.listdir(bucket_loc + '/segmentation_bucket/training/images/')\n",
    "classes = np.array([36,72,108,144,180,216])\n",
    "actualClass = np.array([1,2,1,1,3,4])\n",
    "\n",
    "train_images = np.zeros((512,512,3,len(filelist)))\n",
    "train_masks = np.zeros((512,512,len(filelist)))\n",
    "\n",
    "for i in tqdm(range(len(filelist)), desc=\"Loading training set\", unit=\"img\"):\n",
    "    if \".tif\" in filelist[i]:\n",
    "        train_images[:,:,:,i] = iio.imread(bucket_loc + '/segmentation_bucket/training/images/'+filelist[i]) / 255\n",
    "        train_masks[:,:,i] = iio.imread(bucket_loc + '/segmentation_bucket/training/masks/'+filelist[i])\n",
    "        \n",
    "for i in tqdm(range(len(classes)), desc=\"Processing training masks\", unit=\"class\"):\n",
    "    train_masks[np.where(train_masks==classes[i])] = actualClass[i]-1\n",
    "    \n",
    "# Get a list of all of the validation images\n",
    "filelist = os.listdir(bucket_loc + '/segmentation_bucket/validation/images/')\n",
    "\n",
    "valid_images = np.zeros((512,512,3,len(filelist)))\n",
    "valid_masks = np.zeros((512,512,len(filelist)))\n",
    "for i in tqdm(range(len(filelist)), desc=\"Loading validation set\", unit=\"img\"):\n",
    "    if \".tif\" in filelist[i]:\n",
    "        valid_images[:,:,:,i] = iio.imread(bucket_loc + '/segmentation_bucket/validation/images/'+filelist[i]) / 255\n",
    "        valid_masks[:,:,i] = iio.imread(bucket_loc + '/segmentation_bucket/validation/masks/'+filelist[i])\n",
    "\n",
    "for i in tqdm(range(len(classes)), desc=\"Processing validation masks\", unit=\"class\"):\n",
    "    valid_masks[np.where(valid_masks==classes[i])] = actualClass[i]-1\n",
    "    \n",
    "# Get a list of all of the training images\n",
    "filelist = os.listdir(bucket_loc + '/segmentation_bucket/testing/images/')\n",
    "\n",
    "test_images = np.zeros((512,512,3,len(filelist)))\n",
    "test_masks = np.zeros((512,512,len(filelist)))\n",
    "\n",
    "for i in tqdm(range(len(filelist)), desc=\"Loading testing set\", unit=\"img\"):\n",
    "    if \".tif\" in filelist[i]:\n",
    "        test_images[:,:,:,i] = iio.imread(bucket_loc + '/segmentation_bucket/testing/images/'+filelist[i]) / 255\n",
    "        test_masks[:,:,i] = iio.imread(bucket_loc + '/segmentation_bucket/testing/masks/'+filelist[i])\n",
    "\n",
    "for i in tqdm(range(len(classes)), desc=\"Processing testing masks\", unit=\"class\"):\n",
    "    test_masks[np.where(test_masks==classes[i])] = actualClass[i]-1\n",
    "    \n",
    "# Generate dataset classes\n",
    "trainSet = UNetDataset(image_array=train_images, mask_array=train_masks)\n",
    "validSet = UNetDataset(image_array=valid_images, mask_array=valid_masks)\n",
    "testSet  = UNetDataset(image_array=test_images,  mask_array=test_masks)\n",
    "\n",
    "# Create dataloaders for training process\n",
    "trainLoader = torch.utils.data.DataLoader(dataset=trainSet, shuffle=True, batch_size=1)\n",
    "validLoader = torch.utils.data.DataLoader(dataset=validSet, shuffle=False, batch_size=1)\n",
    "testLoader =  torch.utils.data.DataLoader(dataset=testSet, shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification of dataset class with images\n",
    "---\n",
    "To verify that everything is working as expected prior to the training loop, we can use our newly established `UNetDataset` class to load in random images from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick 4 images at random\n",
    "random_int = np.random.randint(low=0, high=len(filelist), size=4)\n",
    "\n",
    "# Display the images\n",
    "fig, axs = plt.subplots(2, 4, layout=\"tight\", figsize=(15, 8))\n",
    "for i in range(4):\n",
    "    out = trainSet.__getitem__(random_int[i])\n",
    "    axs[0,i].imshow(out['image'].permute(2,1,0))\n",
    "    axs[0,i].set_axis_off()\n",
    "    \n",
    "    axs[1,i].imshow(out['mask'].permute(2,1,0),cmap='jet',clim=(0,3))\n",
    "    axs[1,i].set_axis_off()   \n",
    "\n",
    "# Set up a colormap for reference\n",
    "cax = plt.axes([1.01, 0.1, 0.05, 0.8])\n",
    "cmap = cm.get_cmap('jet', 4) \n",
    "ticks = np.linspace(0,1,9)\n",
    "\n",
    "cbar = plt.colorbar(mappable = cm.ScalarMappable(norm=None, cmap=cmap), cax=cax);\n",
    "cbar.ax.set_yticks(ticks[1:len(ticks)-1:2]);\n",
    "cbar.ax.set_yticklabels(['epidermis','dermis','hair','background']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the U-Net\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to creating a training loop, we will first create a function that will be used to visualize the network training (`update_plots`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define method for updating plot\n",
    "def update_plots(ep_range,train_acc,train_loss,valid_acc,valid_loss):\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    fig, axs = plt.subplots(2, 1, layout=\"tight\", figsize=(10, 10))\n",
    "    axs[0].plot(ep_range,train_acc,'ro-',ep_range,valid_acc,'bo-')\n",
    "    axs[0].set_xlabel('Epoch');\n",
    "    axs[0].set_ylabel('Accuracy [%]');\n",
    "\n",
    "    line = axs[1].plot(ep_range,train_loss,'ro-',ep_range,valid_loss,'bo-')\n",
    "    axs[1].set_xlabel('Epoch');\n",
    "    axs[1].set_ylabel('Loss');\n",
    "    axs[1].legend([line[0],line[1]],['training','validation'], loc='upper right',fontsize='x-large')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize the network, set some training parameters such as the loss function and the optimizer used, as well as define the total number of training epochs. For this submodule, a total of 10 epochs has been chosen due to previous tests with this dataset. In general, the number of epochs is arbitrary and is based on how well the network learns.\n",
    "\n",
    "In your own work, you can determine the number of epochs by observing the accuracy and loss from the training and validation datasets every epoch as the network is trained. If the number of epochs is too high (typically >50), then _overfitting_, also known as memorization, of the training dataset can occur, resulting in a less robust network. Alternatively, if the number of epochs is too low, then the network does not have enough data to become accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and set training parameters\n",
    "model = UNet(n_class=4).to(device);\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "          \n",
    "# Set the number of training epochs\n",
    "total_ep = 10\n",
    "\n",
    "# Set up variable used for plotting\n",
    "ep_range = np.array(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Knowledge Check</b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz('../quiz_files/submodule_03/kc2.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will perform an initial pass of the training and validation dataset with the randomly initialized network, which will help us get a baseline accuracy. For this particular pass of both datasets, the `with torch.no_grad():` line is used to ensure that the network is not learning anything during this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Initial check of accuracy ---#\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # Training images\n",
    "    tot_loss = 0\n",
    "    tot_acc = 0\n",
    "    with tqdm(total=len(trainSet), desc=f'Training (0 / {total_ep})', unit='img') as pbar:\n",
    "        for batch in trainLoader:\n",
    "            # Get a batch of training images/masks from the data loader and send them to the gpu\n",
    "            img = batch['image'].to(device)\n",
    "            mask = batch['mask'].squeeze(1).to(device)\n",
    "\n",
    "            # Forward pass for the network\n",
    "            masks_pred = model(img)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(masks_pred, mask)\n",
    "            tot_loss += loss.item()\n",
    "\n",
    "            # See what the network actually predicted for each pixel using the softmax function\n",
    "            masks_pred = torch.argmax(torch.softmax(masks_pred,1),dim=1).cpu().detach().numpy()\n",
    "            mask = mask.cpu().detach().numpy()\n",
    "\n",
    "            # Calculate an accuracy for the image\n",
    "            acc = np.sum(mask == masks_pred) / np.size(masks_pred)\n",
    "            tot_acc += acc\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix(**{'Loss (batch)': loss.item(),'Accuracy (batch)': acc*100})\n",
    "            pbar.update(batch['image'].shape[0])\n",
    "        pbar.set_postfix(**{'Average loss': tot_loss/len(trainSet),'Average accuracy': (tot_acc/len(trainSet))*100})\n",
    "        \n",
    "        train_acc = np.array((tot_acc/len(trainSet))*100)\n",
    "        train_loss = tot_loss/len(trainSet)\n",
    "        \n",
    "    # Validation loop\n",
    "    tot_loss = 0\n",
    "    tot_acc = 0\n",
    "    with tqdm(total=len(validSet), desc=f'Validation (0 / {total_ep})', unit='img') as pbar:\n",
    "        for batch in validLoader:\n",
    "            # Get a batch of training images/masks from the data loader and send them to the gpu\n",
    "            img = batch['image'].to(device)\n",
    "            mask = batch['mask'].squeeze(1).to(device)\n",
    "\n",
    "            # Forward pass for the network\n",
    "            masks_pred = model(img)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(masks_pred, mask)\n",
    "            tot_loss += loss.item()\n",
    "\n",
    "            # See what the network actually predicted for each pixel using the softmax function\n",
    "            masks_pred = torch.argmax(torch.softmax(masks_pred,1),dim=1).cpu().detach().numpy()\n",
    "            mask = mask.cpu().detach().numpy()\n",
    "\n",
    "            # Calculate an accuracy for the image\n",
    "            acc = np.sum(mask == masks_pred) / np.size(masks_pred)\n",
    "            tot_acc += acc\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix(**{'Loss (batch)': loss.item(),'Accuracy (batch)': acc*100})\n",
    "            pbar.update(batch['image'].shape[0])\n",
    "\n",
    "        pbar.set_postfix(**{'Average loss': tot_loss/len(validSet),'Average accuracy': (tot_acc/len(validSet))*100})\n",
    "        \n",
    "        valid_acc = np.array((tot_acc/len(validSet))*100)\n",
    "        valid_loss = tot_loss/len(validSet)  \n",
    "\n",
    "# Clear the output and print the initial baseline accuracy\n",
    "clear_output(wait=True)\n",
    "print('Initial training accuracy: ' + str(np.round(train_acc,2)) + '%')\n",
    "print('Initial validation accuracy: ' + str(np.round(valid_acc,2)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help interpret these accuracy numbers, you can imagine that an accuracy of 50% corresponds to the network getting the correct classification for each pixel 50% of the time, which is basically a coin flip. \n",
    "\n",
    "Since the network has just been initialized with random numbers, the expected accuracy should be very low (<50%). This provides a good baseline measurement, since the network accuracy after any amount of training should never be lower than this number.\n",
    "\n",
    "Now we can perform the network training and visualize the performance of the network. It is important to keep in mind that the network is only learning from the training dataset, and thus the `with torch.no_grad():` line is used during the assessment of the validation dataset.\n",
    "\n",
    "**NOTE: This network will train for 10 epochs. <em><u>If you are using a CPU, this will take about 10 hours to complete</u></em>. Alternatively, this will take about 10 minutes to complete with a GPU.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Actual training loop ---#\n",
    "for ep in range(total_ep):\n",
    "\n",
    "    # Update plots\n",
    "    update_plots(ep_range, train_acc, train_loss, valid_acc, valid_loss)\n",
    "    \n",
    "    # Training loop\n",
    "    tot_loss = 0\n",
    "    tot_acc = 0\n",
    "    with tqdm(total=len(trainSet), desc=f'Training ({ep + 1} / {total_ep})', unit='img') as pbar:\n",
    "        for batch in trainLoader:\n",
    "            # Get a batch of training images/masks from the data loader and send them to the gpu\n",
    "            img = batch['image'].to(device)\n",
    "            mask = batch['mask'].squeeze(1).to(device)\n",
    "\n",
    "            # Forward pass for the network\n",
    "            masks_pred = model(img)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(masks_pred, mask)\n",
    "            tot_loss += loss.item()\n",
    "\n",
    "            # Zero the optimizer, backpropagate, and step the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # See what the network actually predicted for each pixel using the softmax function\n",
    "            masks_pred = torch.argmax(torch.softmax(masks_pred,1),dim=1).cpu().detach().numpy()\n",
    "            mask = mask.cpu().detach().numpy()\n",
    "\n",
    "            # Calculate an accuracy for the image\n",
    "            acc = np.sum(mask == masks_pred) / np.size(masks_pred)\n",
    "            tot_acc += acc\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix(**{'Loss (batch)': loss.item(),'Accuracy (batch)': acc*100})\n",
    "            pbar.update(batch['image'].shape[0])\n",
    "            \n",
    "        pbar.set_postfix(**{'Average loss': tot_loss/len(trainSet),'Average accuracy': (tot_acc/len(trainSet))*100})\n",
    "        \n",
    "        # Prepare for next update of plots\n",
    "        ep_range = np.append(ep_range, ep+1)\n",
    "        train_acc = np.append(train_acc, np.array((tot_acc/len(trainSet))*100))\n",
    "        train_loss = np.append(train_loss, tot_loss/len(trainSet))\n",
    "        \n",
    "    # Validation loop\n",
    "    tot_loss = 0\n",
    "    tot_acc = 0\n",
    "    with tqdm(total=len(validSet), desc=f'Validation ({ep + 1} / {total_ep}', unit='img') as pbar:\n",
    "        for batch in validLoader:\n",
    "            with torch.no_grad():\n",
    "                # Get a batch of training images/masks from the data loader and send them to the gpu\n",
    "                img = batch['image'].to(device)\n",
    "                mask = batch['mask'].squeeze(1).to(device)\n",
    "\n",
    "                # Forward pass for the network\n",
    "                masks_pred = model(img)\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = criterion(masks_pred, mask)\n",
    "                tot_loss += loss.item()\n",
    "\n",
    "                # See what the network actually predicted for each pixel using the softmax function\n",
    "                masks_pred = torch.argmax(torch.softmax(masks_pred,1),dim=1).cpu().detach().numpy()\n",
    "                mask = mask.cpu().detach().numpy()\n",
    "\n",
    "                # Calculate an accuracy for the image\n",
    "                acc = np.sum(mask == masks_pred) / np.size(masks_pred)\n",
    "                tot_acc += acc\n",
    "            \n",
    "                # Update progress bar\n",
    "                pbar.set_postfix(**{'Loss (batch)': loss.item(),'Accuracy (batch)': acc*100})\n",
    "                pbar.update(batch['image'].shape[0])\n",
    "                \n",
    "        pbar.set_postfix(**{'Average loss': tot_loss/len(validSet),'Average accuracy': (tot_acc/len(validSet))*100})\n",
    "        \n",
    "        # Prepare for next update of plots\n",
    "        valid_acc = np.append(valid_acc, np.array((tot_acc/len(validSet))*100))\n",
    "        valid_loss = np.append(valid_loss, tot_loss/len(validSet))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that training has completed, we will calculate a final accuracy for the training, validation, and most importantly, the testing data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Final check of accuracy ---#\n",
    "with torch.no_grad():\n",
    "\n",
    "    # Update plots\n",
    "    update_plots(ep_range, train_acc, train_loss, valid_acc, valid_loss)\n",
    "    final_train_acc = 0\n",
    "    final_train_loss = 0\n",
    "    final_valid_acc = 0\n",
    "    final_valid_loss = 0\n",
    "    final_test_acc = 0\n",
    "    final_test_loss = 0\n",
    "    \n",
    "    #--- Training dataset ---#\n",
    "    with tqdm(total=len(trainSet), desc=f'Final Training', unit='img') as pbar:\n",
    "        tot_loss = 0\n",
    "        tot_acc = 0\n",
    "    \n",
    "        for batch in trainLoader:\n",
    "            # Get a batch of training images/masks from the data loader and send them to the gpu\n",
    "            img = batch['image'].to(device)\n",
    "            mask = batch['mask'].squeeze(1).to(device)\n",
    "\n",
    "            # Forward pass for the network\n",
    "            masks_pred = model(img)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(masks_pred, mask)\n",
    "            tot_loss += loss.item()\n",
    "\n",
    "            # See what the network actually predicted for each pixel using the softmax function\n",
    "            masks_pred = torch.argmax(torch.softmax(masks_pred,1),dim=1).cpu().detach().numpy()\n",
    "            mask = mask.cpu().detach().numpy()\n",
    "\n",
    "            # Calculate an accuracy for the image\n",
    "            acc = np.sum(mask == masks_pred) / np.size(masks_pred)\n",
    "            tot_acc += acc\n",
    "                \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix(**{'Loss (batch)': loss.item(),'Accuracy (batch)': acc*100})\n",
    "            pbar.update(batch['image'].shape[0])\n",
    "            \n",
    "        pbar.set_postfix(**{'Average loss': tot_loss/len(trainSet),'Average accuracy': (tot_acc/len(trainSet))*100})  \n",
    "        \n",
    "        # Calculate the final loss and accuracy\n",
    "        final_train_loss = tot_loss/len(trainSet)\n",
    "        final_train_acc = (tot_acc/len(trainSet))*100\n",
    "      \n",
    "    #--- Validation dataset ---#\n",
    "    with tqdm(total=len(validSet), desc=f'Final Validation', unit='img') as pbar:\n",
    "        tot_loss = 0\n",
    "        tot_acc = 0\n",
    "        \n",
    "        for batch in validLoader:\n",
    "            # Get a batch of training images/masks from the data loader and send them to the gpu\n",
    "            img = batch['image'].to(device)\n",
    "            mask = batch['mask'].squeeze(1).to(device)\n",
    "\n",
    "            # Forward pass for the network\n",
    "            masks_pred = model(img)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(masks_pred, mask)\n",
    "            tot_loss += loss.item()\n",
    "\n",
    "            # See what the network actually predicted for each pixel using the softmax function\n",
    "            masks_pred = torch.argmax(torch.softmax(masks_pred,1),dim=1).cpu().detach().numpy()\n",
    "            mask = mask.cpu().detach().numpy()\n",
    "\n",
    "            # Calculate an accuracy for the image\n",
    "            acc = np.sum(mask == masks_pred) / np.size(masks_pred)\n",
    "            tot_acc += acc\n",
    "                \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix(**{'Loss (batch)': loss.item(),'Accuracy (batch)': acc*100})\n",
    "            pbar.update(batch['image'].shape[0])\n",
    "            \n",
    "        pbar.set_postfix(**{'Average loss': tot_loss/len(validSet),'Average accuracy': (tot_acc/len(validSet))*100})\n",
    "        \n",
    "        # Calculate the final loss and accuracy\n",
    "        final_valid_loss = tot_loss/len(validSet)\n",
    "        final_valid_acc = (tot_acc/len(validSet))*100\n",
    "        \n",
    "    #--- Testing dataset ---#\n",
    "    with tqdm(total=len(testSet), desc=f'Final Testing', unit='img') as pbar:\n",
    "        tot_loss = 0\n",
    "        tot_acc = 0\n",
    "        \n",
    "        for batch in testLoader:\n",
    "            # Get a batch of training images/masks from the data loader and send them to the gpu\n",
    "            img = batch['image'].to(device)\n",
    "            mask = batch['mask'].squeeze(1).to(device)\n",
    "\n",
    "            # Forward pass for the network\n",
    "            masks_pred = model(img)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(masks_pred, mask)\n",
    "            tot_loss += loss.item()\n",
    "\n",
    "            # See what the network actually predicted for each pixel using the softmax function\n",
    "            masks_pred = torch.argmax(torch.softmax(masks_pred,1),dim=1).cpu().detach().numpy()\n",
    "            mask = mask.cpu().detach().numpy()\n",
    "\n",
    "            # Calculate an accuracy for the image\n",
    "            acc = np.sum(mask == masks_pred) / np.size(masks_pred)\n",
    "            tot_acc += acc\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix(**{'Loss (batch)': loss.item(),'Accuracy (batch)': acc*100})\n",
    "            pbar.update(batch['image'].shape[0])\n",
    "            \n",
    "        pbar.set_postfix(**{'Average loss': tot_loss/len(testSet),'Average accuracy': (tot_acc/len(testSet))*100})  \n",
    "        \n",
    "        # Calculate the final loss and accuracy\n",
    "        final_test_loss = tot_loss/len(testSet)\n",
    "        final_test_acc = (tot_acc/len(testSet))*100\n",
    "\n",
    "# Final output containing plots of final accuracy/loss\n",
    "clear_output(wait=True)\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, layout=\"tight\", figsize=(10, 10))\n",
    "axs[0].plot(ep_range,train_acc,'ro-',ep_range,valid_acc,'bo-')\n",
    "axs[0].plot(total_ep,final_test_acc,'go')\n",
    "axs[0].set_xlabel('Epoch');\n",
    "axs[0].set_ylabel('Accuracy [%]');\n",
    "\n",
    "line = axs[1].plot(ep_range,train_loss,'ro-',ep_range,valid_loss,'bo-',total_ep,final_test_loss,'go-')\n",
    "axs[1].set_xlabel('Epoch');\n",
    "axs[1].set_ylabel('Loss');\n",
    "axs[1].legend([line[0],line[1],line[2]],['training','validation','testing'], loc='upper right',fontsize='x-large')\n",
    "plt.show()\n",
    "    \n",
    "print('Final training accuracy: ' + str(np.round(final_train_acc,2)) + '%')\n",
    "print('Final validation accuracy: ' + str(np.round(final_valid_acc,2)) + '%')\n",
    "print('Final testing accuracy: ' + str(np.round(final_test_acc,2)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the output\n",
    "---\n",
    "After the training is complete, it is a good idea to output some of the images the network produces to check that the network is generating an output that you expect. For the following grid of images, the first row is the input image, the second row is the manually segmented mask, and the third row is the segmented mask generated by the trained CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick 4 images at random\n",
    "random_int = np.random.randint(low=0, high=len(filelist), size=4)\n",
    "\n",
    "# Display the images\n",
    "fig, axs = plt.subplots(3, 4, layout=\"tight\", figsize=(20, 15))\n",
    "for i in range(4):\n",
    "    out = trainSet.__getitem__(random_int[i])\n",
    "    \n",
    "    img = out['image'].to(device)\n",
    "    mask = out['mask'].squeeze(1)\n",
    "\n",
    "    masks_pred = model(img.unsqueeze(0))\n",
    "\n",
    "    masks_pred = torch.argmax(torch.softmax(masks_pred,1),dim=1).cpu()\n",
    "    \n",
    "    axs[0,i].imshow(img.cpu().permute(2,1,0))\n",
    "    axs[0,i].set_axis_off()\n",
    "    \n",
    "    axs[1,i].imshow(mask.permute(2,1,0),cmap='jet',clim=(0,3))\n",
    "    axs[1,i].set_axis_off()\n",
    "\n",
    "    axs[2,i].imshow(masks_pred.permute(2,1,0),cmap='jet',clim=(0,3))\n",
    "    axs[2,i].set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"color:black\"> <b>EXERCISE</b> In the original set of outputs, the network performs relatively well. However, this network can be improved by adjusting training hyperparameters within the loss function, as well as introducing additional training constraints such as schedulers. Try making adjustments to the training loop and assess how they affect the final output. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"color:black\"> <b>CHALLENGE</b> One major drawback to this dataset is that the number of pixels for each class are not balanced (i.e. some classes contains more pixels overall relative to other classes). Within this training dataset, here is the distribution of the classes:\n",
    "<ul>\n",
    "    <li> epidermis: 37.64% </li>\n",
    "    <li> dermis: 18.47% </li>\n",
    "    <li> hair: 3.05% </li>\n",
    "    <li> background 40.84% </li>\n",
    "</ul>\n",
    "\n",
    "Can you adjust some training parameters to take this distribution into consideration?\n",
    "\n",
    "<i>HINT:</i> The CrossEntropyLoss function documentation can be found <a href=https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html>here</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this submodule, we discussed and explored the use of segmentation. This type of segmentation is a powerful tool for generating pixel-wise classification maps, rather than a single classification for an entire input image. To demonstrate this, we generated a U-Net CNN architecture, and trained the network to segment different regions of skin within fluorescence images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "To keep your workspaced organized remember to: \n",
    "\n",
    "1. Save your work.\n",
    "2. Shut down any notebooks and active sessions to avoid extra charges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "---\n",
    "<div> \n",
    "    <span id=\"Reference1\"> \n",
    "        <p> \n",
    "            [1] Cireşan DC, et al.\n",
    "            <i> Deep neural networks segment neuronal membranes in electron microscopy images</i>.\n",
    "            2012.\n",
    "            <a href=\"https://papers.nips.cc/paper/2012/hash/459a4ddcb586f24efd9395aa7662bc7c-Abstract.html\">Link</a> \n",
    "        </p> \n",
    "    </span>   \n",
    "    <span id=\"Reference2\"> \n",
    "        <p> \n",
    "            [2] Ronneberger O, et al. \n",
    "            <i> U-Net: Convolutional Networks for Biomedical Image Segmentation</i>.\n",
    "            2015.\n",
    "            <a href=\"https://arxiv.org/abs/1505.04597\">Link</a> \n",
    "        </p> \n",
    "    </span>\n",
    "</div>\n",
    "<div>\n",
    "    <span id=\"Reference3\"> \n",
    "        <p> \n",
    "            [3] Jones JD, Quinn KP.\n",
    "            <i> Automated Quantitative Analysis of Wound Histology Using Deep-Learning Neural Networks</i>.\n",
    "            2020.\n",
    "            <a href=\"https://doi.org/10.1016/j.jid.2020.10.010\">Link</a> \n",
    "        </p> \n",
    "    </span>\n",
    "</div>\n",
    "<div>\n",
    "    <span id=\"Reference4\"> \n",
    "        <p> \n",
    "            [4] Jones JD, et al.\n",
    "            <i> Quantifying Age-Related Changes in Skin Wound Metabolism Using In Vivo Multiphoton Microscopy</i>.\n",
    "            2020.\n",
    "            <a href=\"https://doi.org/10.1089/wound.2019.1030\">Link</a> \n",
    "        </p> \n",
    "    </span>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
