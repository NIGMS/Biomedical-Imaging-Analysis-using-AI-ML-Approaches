{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Title of Module]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Overview\n",
    "In previous submodules we looked at classification and segmentation to predict the next label. The label is a finite set of distinct values. However, that is not always be the case and the predicted value may need to be continuous and can take on an infinite set of values. In this case we use a method called `Regression`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "+ `Understand the concept of regression`:\n",
    " Learn what regression is, its purpose (estimating relationships between variables), and different types (linear, non-linear, univariate, multivariate).\n",
    "+ `Read and preprocess tabular data`: \n",
    "Learn how to read data from a CSV file using pandas, drop unnecessary columns, and convert categorical data to numerical data for regression.\n",
    "+ `Evaluate regression models`: Understand and apply metrics for evaluating regression models, including Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE).\n",
    "+ `Implement regression using scikit-learn`:  Learn how to use scikit-learn's LinearRegression, DecisionTreeRegressor, and RandomForestRegressor to build and train regression models.\n",
    "+ `Implement regression using PyTorch`: Learn how to build and train a linear regression model using PyTorch. \n",
    "+ `Perform feature selection`: Understand the importance of feature selection and apply techniques like correlation statistics (f_regression) and mutual information statistics (mutual_info_regression) to select the most relevant features for regression modeling using SelectKBest.\n",
    "+ `Visualize and interpret results`:  Learn how to visualize correlation matrices using heatmaps and interpret the results of regression experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "**Data**\n",
    "\n",
    "+ `Kaggle Breast Cancer Dataset`\n",
    "\n",
    "**Main libraries**\n",
    "\n",
    "* `scikit-learn`: this library is specifically designed for data analysis. It includes functions for classification, clustering, and regression as well functions for preprocessing data [2].\n",
    "* `pandas`: this library is for storing and retrieving data.\n",
    "* `numpy`: this library is for converting data into vectors and matrices and performing matrix operations (like multiplication, addition, subtraction, etc.)\n",
    "* `pytorch`: this library focuses on designing machine learning models\n",
    "* `matplotlib` and `seaborn`: libraries used for plotting and visual analysis of data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous submodules we looked at classification and segmentation to predict the next label. The label is a finite set of distinct values. However, that is not always be the case and the predicted value may need to be continuous and can take on an infinite set of values. In this case we use a method called `Regression`. \n",
    "\n",
    "Regression is a statistical process that estimates the relationship between a dependent (also called *response* or *label*) and one or more independent variables (also called *predictors*, *covariates* or *features*).  Regression can be linear or non-linear depending on the relationship between the the dependent and independent variables. Regression is useful for estimating/predicting or forecasting the next value in a sequence and the linear regression can be represented as:\n",
    "\n",
    "$ \\textbf{y} = m\\textbf{x} + c $\n",
    "\n",
    "![Figure 1: reg](reg0.png)\n",
    "\n",
    "In the equation: $ \\textbf{y} $ is the dependent variable and $ \\textbf{x} $ is the independent variable. The object of regression is to estimate $ m $ and $ c $. If there is only one independent variable, then it is referred to as *univariate* regression and if there are multiple variables then it is referred to as *multivariate* regression.\n",
    "\n",
    "Regression is usually implemented on tabular data. For example, we may extract specific features from an image and present them in a tabular form. We could then use the extracted features to predict other values.\n",
    "\n",
    "This tutorial covers basic regression and its concepts. We use Python libraries to implement regression. We will begin the implementation of regression by using a simple example with tabular data. For tabular data we use Pythonâ€™s `scikit-learn` library, a very popular library for data analysis and implementing regression. Regression can also be implemented using machine learning. For implementing regression in machine learning we use the pytorch library. \n",
    "\n",
    "The following topics will be covered in this tutorial:\n",
    "\n",
    "* <a href=\"#reading\">Reading tabular data</a></br>\n",
    "* <a href=\"#scikit-learn\">Regression using ``scikit-learn``</a></br>\n",
    "* <a href=\"#pytorch\">Regression using ``pytorch``</a></br>\n",
    "* <a href=\"#feature\">Feature selection</a></br>\n",
    "* <a href=\"#conclusion\">Conclusion</a></br>\n",
    "* <a href=\"#ref\">References</a></br>\n",
    "* <a href=\"#quiz\">Self assessment</a></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"reading\">Reading tabular data</a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin the tutorial by first reading the tabular data that will be used for demonstrating regression. The tabular data represents breast cancer diagnosis. \n",
    "\n",
    "The tabular data is read in as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To read csv files requires the Pandas library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read in the data\n",
    "df = pd.read_csv('kaggle_breastcancer_data.csv')\n",
    "\n",
    "# IDs column is not required so it is dropped\n",
    "df.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "# Visualize first 4 rows\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we separate the independent variables and dependent variables.\n",
    "\n",
    "The *radius_mean* is the dependent variable which is stored as *label*. All other columns in the table are independent variables which are stored as *features*. The dependent and independent variables can be viewed using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the column we want to predict (dependent variable)\n",
    "TARGET_COLUMN = 'radius_mean'\n",
    "\n",
    "# We drop everything in the dataframe thats not the dependent variable\n",
    "features = df.drop(TARGET_COLUMN, axis=1)\n",
    "\n",
    "label = df[TARGET_COLUMN]\n",
    "\n",
    "# We need to convert \"M\" and \"B\" to numerical vaues 1 and 0 for regression\n",
    "features['diagnosis'] = features['diagnosis'].apply(lambda x: 0 if x == 'B' else 1)\n",
    "\n",
    "# Display the features (indepdent variables) and label (dependent variables)\n",
    "print(\"List of Features: %s\" % ', '.join(features))\n",
    "print(\"\\nLabel: %s\" % TARGET_COLUMN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for model evaluation\n",
    "---\n",
    "\n",
    "Once a regression model is created, the next task is to determine how well the model works. There are many different metrics that can be used to measure the effectiveness of the model. \n",
    "* **Mean Absolute Error (MAE)**: measures the difference between the actual and the predicted values (also called the *residual*) and then computes the mean.\n",
    "\n",
    "MAE = $\\frac{1}{N}\\sum \\limits_{i=0}^{N} (y_i - \\hat{y_i})^2$\n",
    "* **Mean Squared Error (MSE)**: measures the square of the difference between the actual and the predicted values and then computes the mean.\n",
    "\n",
    "MSE = $\\frac{1}{N}\\sum \\limits_{i=0}^{N} (y_i - \\hat{y_i})$\n",
    "* **Root Mean Square Error (RMSE)**: measures the mean of the square of the residual and then computes the square root of that mean.  \n",
    "\n",
    "RMSE = $\\sqrt{\\frac{1}{N}\\sum \\limits_{i=0}^{N} (y_i - \\hat{y_i})^2}$\n",
    "\n",
    "Where $y_i$ is the actual ith variable, $\\hat{y_i}$ is the estimated ith variable, $N$ is the number of points.\n",
    "\n",
    "Ideally, the MAE, MSE and RMSE should be as low as possible.\n",
    "\n",
    "To evaluate the model, we use mean functions from ``scikit-learn`` to calculate these metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"scikit-learn\">Regression using ``scikit-learn``</a>\n",
    "---\n",
    "\n",
    "We create a basic regression function. The function takes in input regression model, features which are the independent variables, labels which are dependent variables and the number of iterations over which the model is repeatedly run.\n",
    "Within the function, the features and labels are divided into training and testing set. We use the ``train_test_split()`` function from ``scikit-learn`` library. The testing size is $1/3$ of the data size. The training set is used for training the regression model and the testing set is used for evaluating the regression model.\n",
    "We use MAE and RMSE metrics to measure the effectiveness of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def SciKitRegressionModel(model, features, label, iterations = 100):\n",
    "    \n",
    "    '''\n",
    "        Function for running Regression using SciKit-learn library\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : SciKit-learn regression function, e.g. LinearRegression(), DecisionTreeRegressor(), RandomForestRegressor\n",
    "        features : independent variable, matrix of shape = [dim, n_features]\n",
    "        label : dependent variable, vector of shape = [dim, ]\n",
    "        iterations : number of iterations to run\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        maeReg : MAE values from regression model after evaluation, list of shape = [iterations, ]\n",
    "        rmseReg: RMSE values from regression model after evaluation, list of shape = [iterations, ]\n",
    "    ''' \n",
    "    \n",
    "    maeReg, rmseReg, r2Reg = [], [], []\n",
    "\n",
    "    for i in range(iterations):\n",
    "\n",
    "        RegMod = model\n",
    "        XTrain, XTest, yTrain, yTest = train_test_split(features, label, test_size=1 / 3)\n",
    "        RegMod.fit(XTrain, yTrain)\n",
    "\n",
    "        reslist = RegMod.predict(XTest).tolist()\n",
    "        truthlist = yTest.tolist()\n",
    "\n",
    "        mae = mean_absolute_error(reslist, truthlist)\n",
    "        rmse = mean_squared_error(reslist, truthlist) ** .5\n",
    "\n",
    "        maeReg.append(mae)\n",
    "        rmseReg.append(rmse)\n",
    "\n",
    "    print(\"Model    : \" + str(model))\n",
    "    print(\"MAE      : Mean %.4f Deviation %.4f\" % (np.mean(maeReg), np.std(maeReg)))\n",
    "    print(\"RMSE     : Mean %.4f Deviation %.4f\" % (np.mean(rmseReg), np.std(rmseReg)))\n",
    "    \n",
    "    return maeReg, rmseReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "maeLin, rmseLin = SciKitRegressionModel(LinearRegression(), features, label, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the linear regression model in ``scikit-learn``, there are many other regression methods, like logistic regression, ensemble methods, etc. For this tutorial, we will focus on ensemble methods. The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm to improve generalizability / robustness over a single estimator. In this tutorial we will focus on using decision tree and random forest ensemble methods for regression.\n",
    "\n",
    "Decision trees create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree is built through a process known as binary recursive partitioning, which is an iterative process that splits the data into partitions or branches, and then continues splitting each partition into smaller groups as the method moves up each branch. The random forest regressor is a randomized version of the decision tree. Each tree in the ensemble is built from a sample drawn with replacement from the training set.\n",
    "\n",
    "We will use the ``DecisionTreeRegressor`` and ``RandomForestRegressor`` function from ``scikit-learn`` to implement the random forest ensemble regression [2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "maeDtree, rmseDTree = SciKitRegressionModel(DecisionTreeRegressor(), features, label, 100)\n",
    "maeRfr, rmseRfr = SciKitRegressionModel(RandomForestRegressor(), features, label, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"pytorch\">Regression using ``pytorch``</a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression model can also be implemented using `pytorch`. To implement regression using Pytorch we use the `nn.Linear`.\n",
    "\n",
    "The `nn.Linear(a,b)` is a module that creates single layer feed forward network with `a` inputs and `b` outputs. Mathematically, this module is designed to calculate the linear equation $ \\textbf{y} = m\\textbf{x} + c $ where $ \\textbf{x} $ is input, $ \\textbf{y} $ is output, $m$ is weight and $c$ is the bias. For our case `a` is the number of features of the data frame and `b` is 1 which is the number of labels we are estimating. The weight and bias are obtained using the training phase.\n",
    "\n",
    "The model also requires defining of some parameters which are learning rate, loss function and the optimizer. \n",
    "\n",
    "Before we implement Regression model in Pytorch, we will perform some preprocessing. This includes splitting the data into training and testing tests and converting the pandas dataframe into a numpy array. The numpy array is then normalized. The goals of normalization are to change the values to a common scale and reduce the effect of outliers. Normalization improves the numerical stability of the model. We use the popular *min-max* normalization. *min-max* is defined as:\n",
    "\n",
    "$ x[:,i] = \\frac{x[:,i] - min(x[:,i])}{max(x[:,i]) - min(x[:,i])} $ \n",
    "\n",
    "where $i$ is the $ith$ value of $x$.\n",
    "Normalization can also be implemented using the ``MinMaxScaler()`` from scikit-learn. However, we design our custom function for nomalization for understanding how the min-max normalization takes place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "\n",
    "featuresnp = features.to_numpy()\n",
    "labelnp = label.to_numpy()\n",
    "\n",
    "XTrainNP, XTestNP, yTrainNP, yTestNP = train_test_split(featuresnp, labelnp, test_size=1 / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization function\n",
    "def normalizer(x, y):\n",
    "    \n",
    "    '''\n",
    "        Function to perform normalization\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : numpy array to normalize, matrix of shape [dim, n_features]\n",
    "        y : numpy array used to normalize x, matrix of shape [dim, n_features]\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        normX : normalized x values between 0 and 1, matrix of shape [dim, n_features]\n",
    "    ''' \n",
    "    \n",
    "    minRange = np.min(y, axis=0)\n",
    "    maxRange = np.max(y, axis=0)\n",
    "    normX = (x - minRange) / (maxRange - minRange)\n",
    "    return normX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>EXERCISE</b> We use a custom function for normalization. Try Using the MinMaxScaler() function to perform normalization and use the normalized data as input to the Pytorch model. Is the result the same?\n",
    "\n",
    "The function can be implemented using the following commands:\n",
    "    \n",
    "from **sklearn.preprocessing** import **MinMaxScaler**\n",
    "    \n",
    "$sc = MinMaxScaler()$\n",
    "    \n",
    "normalizedData = sc.fit_transform(data) </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrainNorm = normalizer(XTrainNP, XTrainNP)\n",
    "yTrainNorm = normalizer(yTrainNP, yTrainNP).reshape(-1,1)\n",
    "\n",
    "XTestNorm = normalizer(XTestNP, XTrainNP)\n",
    "yTestNorm = normalizer(yTestNP, yTrainNP).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Knowledge Check</b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jupyterquiz==2.0.7 --quiet\n",
    "from jupyterquiz import display_quiz\n",
    "display_quiz('../quiz_files/submodule_04/kc1.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "---\n",
    "\n",
    "First we define a Torch model using Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not already installed: \n",
    "# !pip install tqdm torchshow torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "\n",
    "input_size = features.shape[1]\n",
    "output_size = 1\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    \n",
    "     # Class for implementing Linear Regression in Pytorch\n",
    "\n",
    "    def __init__(self, input_size , output_size):\n",
    "        \n",
    "        '''\n",
    "        Function for creating linear regression model\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size : input size for model, integer = n_features\n",
    "        output_size : output size of model, integer = n_features\n",
    "        ''' \n",
    "        \n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size , output_size)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        '''\n",
    "        Function for creating linear regression model\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : indepedent variable, matrix of shape [dim, n_features]\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        y_pred : predicted labels, vector of shape [dim, ]\n",
    "        ''' \n",
    "        \n",
    "        y_pred = self.linear(x)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "model = LinearRegressionModel(input_size , output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enter data into a Pytorch model, it has to be converted into a tensor object. In the next cell we convert the numpy data into torch tensor objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy to torch for training the model\n",
    "XTrainTensor = torch.from_numpy(XTrainNorm.astype(np.float32))\n",
    "yTrainTensor = torch.from_numpy(yTrainNorm.astype(np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the regression model from before, the machine learning model also has to be trained. We define a training function that trains on training data by minimizing MSE. The training is done using an algorithm called *gradient descent*. Gradient descent is an iterative algorithm that finds a local minimum of a differentiable function. The idea is to take small incremental steps to measure an approximate gradient at each point which is the minimum of the loss function. The incremental steps are determined by the parameter *learning rate*.\n",
    "\n",
    "The weights and bias are updated accordingly. The gradient descent is implement using the following steps:\n",
    "\n",
    "* Determine the loss function (in this case we are using MSE)\n",
    "* Calculate the gradient of the loss with respect to the independent variables by using the command ``.backward()``. This is referred to as *backpropagation*\n",
    "* Update the weights and bias using the ``step()`` command\n",
    "* Repeat the above steps\n",
    "\n",
    "At each training phase, we measure the MSE loss. The loss reduces as we train for more epochs. However, after a certain number of epochs, the loss will plateau and not reduce any further. At this point it will not be useful to train the model for any more epochs and in most cases the training is stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate )\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "maeML, rmseML = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #forward feed\n",
    "    y_pred = model(XTrainTensor.requires_grad_())\n",
    "\n",
    "    #calculate the loss\n",
    "    loss = loss_function(y_pred, yTrainTensor)\n",
    "    \n",
    "    mae = mean_absolute_error(y_pred.detach().numpy(), yTrainTensor.detach().numpy())\n",
    "    rmse = mean_squared_error(y_pred.detach().numpy(), yTrainTensor.detach().numpy()) ** .5\n",
    "    \n",
    "    maeML.append(mae)\n",
    "    rmseML.append(rmse)\n",
    "\n",
    "    #backward propagation: calculate gradients\n",
    "    loss.backward()\n",
    "\n",
    "    #update the weights\n",
    "    optimizer.step()\n",
    "\n",
    "    #clear out the gradients from the last step loss.backward()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model\n",
    "---\n",
    "\n",
    "In this section we test the model trained in the previous section. The test set is converted to numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy to torch for evaluating the model\n",
    "XTestTensor = torch.from_numpy(XTestNorm.astype(np.float32))\n",
    "yTestTensor = torch.from_numpy(yTestNorm.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTestHat = model(XTestTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maeMLTest = mean_absolute_error(yTestHat.detach().numpy(), yTestTensor.detach().numpy())\n",
    "rmseMLTest = mean_squared_error(yTestHat.detach().numpy(), yTestTensor.detach().numpy()) ** .5\n",
    "\n",
    "print(\"Model    : Pytorch Linear Regression\")\n",
    "print(\"MAE      : Mean %.4f\" % (maeMLTest))\n",
    "print(\"RMSE     : Mean %.4f\"  % (rmseMLTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>EXERCISE</b> Learning rate is an important hyperparamter that effects training speed and accuracy of the model. Try changing the learning rate (lr) to 0.01 and 0.1. How does it effect MAE and RMSE? </div>\n",
    "\n",
    "Below is a complete function that runs the entire Pytorch model and returns MAE and RMSE. We combine all the functions from before and run the entire code 100 times to create a different train-test split, which results in different MAE and RMSE values. Finally, we take the average of MAE and RMSE over the 100 iterations to get the MAE and RMSE values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PytorchRegressionModel(features, label, iterations = 100):\n",
    "    \n",
    "    '''\n",
    "        Function for running Regression using Pytorch\n",
    "        Parameters\n",
    "        ----------\n",
    "        features : independent variable, numpy matrix of shape = [dim, n_features]\n",
    "        label : dependent variable, numpy vector of shape = [dim, ]\n",
    "        iterations : number of iterations to run\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        maeReg : MAE values from regression model after evaluation, list of shape = [iterations, ]\n",
    "        rmseReg: RMSE values from regression model after evaluation, list of shape = [iterations, ]\n",
    "    ''' \n",
    "    \n",
    "    input_size = features.shape[1]\n",
    "    output_size = 1\n",
    "    \n",
    "    maeML, rmseML = [], []\n",
    "    model = LinearRegressionModel(input_size , output_size)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        \n",
    "        # Split the data into training and testing sets\n",
    "        XTrainNP, XTestNP, yTrainNP, yTestNP = train_test_split(features, label, test_size=1 / 3)\n",
    "\n",
    "        XTrainNorm = normalizer(XTrainNP, XTrainNP)\n",
    "        yTrainNorm = normalizer(yTrainNP, yTrainNP).reshape(-1,1)\n",
    "\n",
    "        XTestNorm = normalizer(XTestNP, XTrainNP)\n",
    "        yTestNorm = normalizer(yTestNP, yTrainNP).reshape(-1,1)\n",
    "\n",
    "        # Convert numpy to torch for training the model\n",
    "        XTrainTensor = torch.from_numpy(XTrainNorm.astype(np.float32))\n",
    "        yTrainTensor = torch.from_numpy(yTrainNorm.astype(np.float32))\n",
    "       \n",
    "        learning_rate = 0.0001\n",
    "        loss_function = nn.MSELoss()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate )\n",
    "\n",
    "        num_epochs = 100\n",
    "\n",
    "        maeML, rmseML = [], []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            #forward feed\n",
    "            y_pred = model(XTrainTensor.requires_grad_())\n",
    "\n",
    "            #calculate the loss\n",
    "            loss = loss_function(y_pred, yTrainTensor)\n",
    "\n",
    "            mae = mean_absolute_error(y_pred.detach().numpy(), yTrainTensor.detach().numpy())\n",
    "            rmse = mean_squared_error(y_pred.detach().numpy(), yTrainTensor.detach().numpy()) ** .5\n",
    "\n",
    "            maeML.append(mae)\n",
    "            rmseML.append(rmse)\n",
    "\n",
    "            #backward propagation: calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            #update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            #clear out the gradients from the last step loss.backward()\n",
    "            optimizer.zero_grad()\n",
    "            #if epoch % 100 == 0:\n",
    "                #print('epoch {}, loss {}'.format(epoch, loss.item()))\n",
    "\n",
    "        # Convert numpy to torch for evaluating the model\n",
    "        XTestTensor = torch.from_numpy(XTestNorm.astype(np.float32))\n",
    "        yTestTensor = torch.from_numpy(yTestNorm.astype(np.float32))\n",
    "\n",
    "        yTestHat = model(XTestTensor)\n",
    "\n",
    "        maeMLTest = mean_absolute_error(yTestHat.detach().numpy(), yTestTensor.detach().numpy())\n",
    "        rmseMLTest = mean_squared_error(yTestHat.detach().numpy(), yTestTensor.detach().numpy()) ** .5\n",
    "        \n",
    "        maeML.append(maeMLTest)\n",
    "        rmseML.append(rmseMLTest)\n",
    "                         \n",
    "    print(\"Model    : \" + 'Pytorch Regression')\n",
    "    print(\"MAE      : Mean %.4f Deviation %.4f\" % (np.mean(maeML), np.std(maeML)))\n",
    "    print(\"RMSE     : Mean %.4f Deviation %.4f\" % (np.mean(rmseML), np.std(rmseML)))\n",
    "        \n",
    "    return maeML, rmseML\n",
    "\n",
    "maeML, rmseML = PytorchRegressionModel(features.to_numpy(), label.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\"> <b>CHALLENGE</b> Use more linear layers in the model. Observe how MAE and RMSE change. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Knowledge Check</b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz('../quiz_files/submodule_04/kc2.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"feature\">Feature selection</a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature selection** is the process of identifying and selecting a subset of input variables that are most relevant to the target variable.\n",
    "\n",
    "The simplest case of feature selection is one which involves numerical input variables and a numerical target for regression predictive modeling. This is because the strength of the relationship between each input variable and the target can be calculated (referred to as correlation) and compared to each other. By selecting the features that are highly correlated, the MAE and RMSE can be significantly reduced.\n",
    "\n",
    "Feature selection can be done using machine learning. However, here we will use the ``scikit-learn`` library for feature selection. There are two popular feature selection techniques that can be used for numerical input data and a numerical target variable.\n",
    "\n",
    "* Correlation Statistics: Correlation Statistics is a measure of how closely two variables change together. The larger the relationship, the more likely the feature can be selected for modeling. In Python, this is implemented using the ``f_regression()`` function.\n",
    "* Mutual Information Statistics: Mutual information is calculated between two variables and measures the reduction in uncertainty for one variable given a known value of the other variable. In Python, this is implemented using the ``mutual_info_regression()`` function.\n",
    "\n",
    "We create a function that can be used for feature extraction. The function takes in as input our training data, testing data, the label, the type of feature selection function to use and number of top features to extract ($k$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "\n",
    "# feature selection\n",
    "def select_features(features, label, score_function, k = 5):\n",
    "    \n",
    "    '''\n",
    "        Function for selecting top k features\n",
    "        Parameters\n",
    "        ----------\n",
    "        features : independent variable, numpy matrix of shape = [dim, n_features]\n",
    "        label : dependent variable, numpy vector of shape = [dim, ]\n",
    "        score_function : function to use for feature selection, can be f_regression() or mutual_info_regression()\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        features_fs : independent variable, numpy matrix of shape = [dim, k]\n",
    "        fs: output object from SelectKBest() function\n",
    "    ''' \n",
    "        \n",
    "    # configure to select all features\n",
    "    fs = SelectKBest(score_func=score_function, k=k)\n",
    "    # learn relationship from data\n",
    "    fs.fit(features, label)\n",
    "    # transform input data\n",
    "    features_fs = fs.transform(features)\n",
    "    return features_fs, fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform feature extraction in this tutorial using the ``f_regression()`` function. However, the function can easily be applied using the ``mutual_info_regression()`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_fs, fs = select_features(features, label, f_regression, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "for i in range(len(fs.scores_)):\n",
    "    print('Feature %d: %f' % (i, fs.scores_[i]))\n",
    "# plot the scores\n",
    "pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select the top 5 features by selecting the features with the highest values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "corr_matrix = df.corr(method=\"pearson\")\n",
    "fig, ax = plt.subplots(figsize=(17,17)) \n",
    "sns.heatmap(corr_matrix, cmap=\"YlGnBu\", annot=True, cbar=True, linewidths=0.5, ax=ax)\n",
    "# sns.heatmap(corr_matrix, vmin=-1., vmax=1., annot=True, fmt='.2f', cmap=\"YlGnBu\", cbar=True, linewidths=0.5)\n",
    "plt.title(\"pearson correlation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation can also be viewed by plotting the correlation matrix as a heatmap. The dark blue colors show strong correlation between the data. This is the same result when we selected the top 5 features using the feature selection function. We will now use this data for training and evaluating our regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maeLinfs, rmseLinfs = SciKitRegressionModel(LinearRegression(), features_fs, label, 100)\n",
    "maeDtreefs, rmseDTreefs = SciKitRegressionModel(DecisionTreeRegressor(), features_fs, label, 100)\n",
    "maeRfrfs, rmseRfrfs = SciKitRegressionModel(RandomForestRegressor(), features_fs, label, 100)\n",
    "maeMLfs, rmseMLfs = PytorchRegressionModel(features_fs,  label.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that reducing the number of features and considering only the top $k$ features reduced the MAE and RMSE. In case of linear regression, the MAE and RMSE increased. This is because the model was already optimal and thus, removing the features did not improve the performance. \n",
    "\n",
    "We summarize all the MAE and RMSE values for all the experiments and present them in a boxplot. The boxplot shows the MAE and RMSE values across all the experiments. The red line is the mean of the MAE and RMSE values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(7,7))\n",
    "axs[0, 0].boxplot([maeLin, maeDtree, maeRfr, maeML], labels=['Linear', 'DTree', 'RF', 'Pytorch'])\n",
    "axs[0, 0].set_title('MAE all Features')\n",
    "axs[0, 1].boxplot([maeLinfs, maeDtreefs, maeRfrfs, maeMLfs], labels=['Linear', 'DTree', 'RF', 'Pytorch'])\n",
    "axs[0, 1].set_title('MAE top k Features')\n",
    "axs[1, 0].boxplot([rmseLin, rmseDTree, rmseRfr, rmseML], labels=['Linear', 'DTree', 'RF', 'Pytorch'])\n",
    "axs[1, 0].set_title('RMSE all Features')\n",
    "axs[1, 1].boxplot([rmseLinfs, rmseDTreefs, rmseRfrfs, rmseMLfs], labels=['Linear', 'DTree', 'RF', 'Pytorch'])\n",
    "axs[1, 1].set_title('RMSE top k Features')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>EXERCISE</b> Implement feature extraction using ``mutual_info_regression()``. Are the top 5 features the same? How does it affect MAE and RMSE? What if the value of k is increased from 5 to 10. How does it effect MAE and RMSE?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Knowledge Check</b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz('../quiz_files/submodule_04/kc3.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this module, we implemented regression using different methods. We studied tabular data and estimated the radius of a cancerous tumor. We looked at how to extract the most relevant features can be extracted and how these features effect the MAE and RMSE metrics. The feature extraction significantly improved MAE and RMSE for random forest, decision tree and pytorch regression models but RMSE and MAE for linear regression was increased. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "To keep your workspaced organized remember to: \n",
    "\n",
    "1. Save your work.\n",
    "2. Shut down any notebooks and active sessions to avoid extra charges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "---\n",
    "[1] Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, Bingbing Ni. \"MedMNIST v2: A Large-Scale Lightweight Benchmark for 2D and 3D Biomedical Image Classification\". arXiv preprint arXiv:2110.14795, 2021.\n",
    "\n",
    "[2] Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
