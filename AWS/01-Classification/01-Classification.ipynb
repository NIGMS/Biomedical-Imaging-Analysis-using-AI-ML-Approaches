{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Overview\n",
    "The notebook provides a comprehensive guide on classification using deep learning, with a focus on practical implementation and comparison of different training approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "+ **Classification**\n",
    "    - ``Overview``: Introduction to classification and its importance, particularly in image recognition.\n",
    "    - ``Focus``: Using deep learning algorithms for classification.\n",
    "+ **Deep Learning**\n",
    "    - ``Definition``: Explanation of deep learning and its advantages over traditional classification methods.\n",
    "    - ``Architecture``: Description of neural networks, including the significance of multiple layers.\n",
    "    - ``Training``: Discussion on training deep learning networks using known labels and testing on unknown labels.\n",
    "    - ``Experimentation``: Mention of experimenting with ResNet architecture using the ImageNet and PathMNIST datasets.\n",
    "+ **Transfer Learning**\n",
    "    - ``Definition``: Explanation of transfer learning and its benefits.\n",
    "    - ``Advantages``: Reasons for using transfer learning, such as the need for large datasets and computational resources.\n",
    "    - ``Objective``: Conducting experiments to demonstrate the effectiveness of transfer learning using ResNet-18.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <i class=\"fa fa-hand-paper-o\" aria-hidden=\"true\"></i>\n",
    "    <b>Tip: </b>  If you're having trouble with any part of this tutorial, feel free to leverage AWS Bedrock (Amazon's advanced generative AI tool) at the bottom of this module.\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "**Data**\n",
    "- ``PathMNIST Dataset``: Automatically downloaded and prepared using the Prepare_Data function.\n",
    "- ``ImageNet Pre-trained Model``: Utilized for transfer learning experiments.\n",
    "\n",
    "**Library Dependencies**\n",
    "\n",
    "* ``pytorch`` and ``torchvision``: these libraries focus on designing machine learning models.\n",
    "* ``medmnist``: this library is specifically designed for reading and processing the MedMNIST dataset. It includes functions for data preperation and formating.\n",
    "* ``tqdm``: a library used to display the progress of code loops.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is the process of categorizing a set of data into separate classes. Image recognition, the task of identifying the label corresponding to an image, is crucial for classification and can become difficult as number of classes increase. This tutorial will focus on using deep learning algorithms for classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Deep Learning?\n",
    "---\n",
    "<img src=\"./Images/1.gif\" alt=\"drawing\" width=\"1000\"/>\n",
    "\n",
    "Deep learning is a machine learning method used to train a computer to recognize patterns in data. Deep learning is useful for classification because it is customized for the training data making it more accurate than traditional classification methods. This method consists of a neural network with three or more layers. These neural networks attempt to simulate the behavior of the human brain “learning” from large amounts of data. While a neural network with a single layer can still make approximate predictions, additional hidden layers can help to optimize and refine for accuracy. However, adding more hidden layers will reduce the training speed because the network will have more parameters (weights) to calibrate. \n",
    "\n",
    "Deep learning networks are typically trained on data for which the labels are known, then tested on data for which labels are unknown from the same dataset. \n",
    "\n",
    "In this submodule we will be experimenting with the [ResNet](https://arxiv.org/abs/1512.03385) deep learning architecture using the [1000-class Imagenet dataset](https://image-net.org/download.php) which is a large collection of images of everyday objects and [PathMNIST](https://medmnist.com/) which is a collection of pathology images. We will be using a popular technique in deep learning for classification named transfer learning.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Transfer Learning?\n",
    "---\n",
    "\n",
    "Transfer learning (TL) focuses on gaining knowledge from one dataset and using it to classify data from another dataset. It leverages the already existing data so the network can quickly achieve good accuracy.\n",
    "<img src=\"./Images/2.jpg\" alt=\"drawing\" width=\"1000\"/>\n",
    "\n",
    "### Why transfer learning?\n",
    "---\n",
    "\n",
    "Suppose you have 100 images of cats and 100 dogs and want to build a model to classify the images. This is a relatively small dataset and the network trained on this dataset would not be able to learn all the information about the data. Hence, the trained network would be less accurate on testing dataset. Enter transfer learning. There are two big reasons why you want to use transfer learning:\n",
    "\n",
    "1) Training models with high accuracy requires a lot of data. For example, the ImageNet dataset contains over 1 million images. In the real world, you are unlikely to have such a large dataset. \n",
    "\n",
    "2) Assuming that you had that kind of dataset, you might still not have the resources required to train a model on such a large dataset. \n",
    "\n",
    "Hence transfer learning makes a lot of sense if you don’t have the compute resources needed to train models on huge datasets. Even if you had the compute resources at your disposal, you still must wait for days or weeks to train such a model. Therefore, using a pre-trained model will save you precious time. \n",
    "\n",
    "\n",
    "### Objective\n",
    "\n",
    "To elaborate further about the effectiveness of transfer learning we will conduct 3 experiments using ResNet-18 model. This model consists of a feature extractor and a classifier. The feature extractor extracts relevant information from the images that the classifier uses for decision making (classification). \n",
    "\n",
    "The <a href=\"#A\">first experiment</a> involves training the classifier without using transfer learning. This means that all the weights are initialized randomly. All the layers of the model are trainable which means that all the parameters (weights) of the model is being optimized. \n",
    "\n",
    "In the <a href=\"#B\">second experiment</a>, we introduce transfer learning with feature extraction. We use ImageNet-1000 dataset to train the feature extractor only. We then use the trained feature extractor that leverages the information learned from ImageNet to extract the features from the new dataset (PathMNIST). The classifier is modified and trained on PathMNIST.\n",
    "\n",
    "Finally, in the <a href=\"#C\">third experiment</a>, we introduce transfer learning with feature extraction finetuning. We use ImageNet-1000 dataset to train the feature extractor. The main difference between this experiment and the previous one is the finetuning meaning that the feature extractor is not fixed and will be trained on PathMNIST. The classifier is modified and trained on PathMNIST.\n",
    "\n",
    "### Notebook workflow:\n",
    "---\n",
    "\n",
    "- <a href=\"#0\">Install and Load Dataset</a></br>\n",
    "- <a href=\"#A\">A. Training ``ResNet`` from scratch</a></br>\n",
    "    1. <a href=\"#A1\">Create ResNet-18 model with random initialization and set the weights to be learned from scratch.</a></br>\n",
    "    2. <a href=\"#A2\">Train on training dataset.</a></br>\n",
    "    3. <a href=\"#A3\">Evaluate on testing dataset.</a></br>\n",
    "- <a href=\"#B\">B. Training ``ResNet`` using Transfer Learning with Feature Extraction</a></br>\n",
    "    1. <a href=\"#B1\">Create ``ResNet-18`` Model pretrained on ``ImageNet`` and turn on learning only for final layers.</a></br>\n",
    "    2. <a href=\"#B2\">Train on training dataset.</a></br>\n",
    "    3. <a href=\"#B3\">Evaluate on testing dataset.</a></br>\n",
    "- <a href=\"#C\">C. Training ``ResNet`` using Transfer Learning without Feature Extraction</a></br>\n",
    "    1. <a href=\"#C1\">Create ``ResNet-18 Model`` pretrained on ``ImageNet`` and turn on learning on all Layers.</a></br>\n",
    "    2. <a href=\"#C2\">Train on training dataset.</a></br>\n",
    "    3. <a href=\"#C3\">Evaluate on testing dataset.</a></br>\n",
    "- <a href=\"#1\">Results</a></br>   \n",
    "- <a href=\"#2\">Conclusion</a></br> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"0\">Install and Load Dataset</a> \n",
    "---\n",
    "``PathMNIST`` is mainly used to predict survival in cancer histological sections. They are Images of size 3 × 28 × 28 pixels. The PathMNIST dataset consists of 9 classes and the number of images are 107,180 split into (training / validation / testing) as (89,996 / 10,004 / 7,180) respectivly.\n",
    "\n",
    "In the following steps we demonstrate how download the PathMNIST dataset and install the library used to manipulate it.\n",
    "\n",
    "We start by running pip installer to install necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "# Setuptools is set to version 70.0.0 due later versions having conflicts/errors \n",
    "!pip install --upgrade setuptools==70.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install medmnist torchshow torchvision torch tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PrepareDataset import Prepare_Data, Get_DataSet_Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Loops import train_loop,test_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model import Create_Model_Optimizer_Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Train_Loader,Val_Loader,Test_Loader,Train_Evaluator,Test_Evaluator) = Prepare_Data(data_flag = 'pathmnist', download = True, batch_size = 128)\n",
    "(Task, Num_Classes) = Get_DataSet_Information(data_flag = 'pathmnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Knowledge Check</b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jupyterquiz==2.0.7 --quiet\n",
    "from jupyterquiz import display_quiz\n",
    "display_quiz('../quiz_files/submodule_01/kc1.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"A\">A. Training ``ResNet`` from scratch</a> \n",
    "---\n",
    "In the following steps we demonstrate how to train a ResNet Model from scratch. The weights for all the layers are initialized randomly and learned throughout the training process only on PathMNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    1. <a name=\"A1\">Create ``ResNet-18`` model with random initialization and set the weights to be learned from scratch.</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Model, Optimizer, Criterion) = Create_Model_Optimizer_Criterion(n_classes = Num_Classes, feature_extract = False, use_pretrained = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    2. <a name=\"A2\">Train on training dataset.</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scratch = train_loop(Model, Train_Loader, Val_Loader, Criterion, Optimizer, Train_Evaluator, num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    3. <a name=\"A3\">Evaluate on testing dataset.</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ScratchMetric = test_loop(Model,Test_Loader,Test_Evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"B\">B. Training ``ResNet`` using Transfer Learning with Feature Extraction</a> \n",
    "---\n",
    "In the following steps we demonstrate how to use a ResNet Model pretrained on ImageNet and retrain the last layer on the PathMNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "####    1. <a name=\"B1\">Create ``ResNet-18`` Model pretrained on ``ImageNet`` and turn on learning only for final layers.</a> \n",
    "\n",
    "Note that we set feature_extract = True and use_pretrained=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Model, Optimizer, Criterion) = Create_Model_Optimizer_Criterion(n_classes = Num_Classes, feature_extract = True, use_pretrained = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    2. <a name=\"B2\">Train on training dataset.</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinalLayers = train_loop(Model, Train_Loader, Val_Loader, Criterion, Optimizer, Train_Evaluator, num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    3. <a name=\"B3\">Evaluate on testing dataset.</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinalLayerMetric = test_loop(Model,Test_Loader,Test_Evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"C\">C. Training ``ResNet`` using Transfer Learning without Feature Extraction</a> \n",
    "---\n",
    "In the following steps we demonstrate how to use a pretrained ResNet Model on ImageNet and train the all layers on PathMNIST. In this case the weights of ResNet model are transfered from ImageNet dataset and used as a starting weights for learning on PathMNIST. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    1. <a name=\"C1\">Create ``ResNet-18`` Model pretrained on ImageNet and turn on learning on all Layers</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Model, Optimizer, Criterion) = Create_Model_Optimizer_Criterion(n_classes = Num_Classes, feature_extract = False, use_pretrained = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    2. <a name=\"C2\">Train on training dataset.</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FineTuned = train_loop(Model, Train_Loader, Val_Loader, Criterion, Optimizer, Train_Evaluator, num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    3. <a name=\"C3\">Evaluate on testing dataset.</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FineTunedMetric = test_loop(Model,Test_Loader,Test_Evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Knowledge Check</b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz('../quiz_files/submodule_01/kc2.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"1\">Results</a> \n",
    "---\n",
    "We start by comparing the accuracies during training of the three experiments. We compare the testing accuracies on PathMNIST. Accuracy is a widely used metric that measures how well the predicted labels match the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training curves of validation accuracy vs. number \n",
    "#  of training epochs for the transfer learning methods and\n",
    "#  the model trained from scratch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "ohist = []\n",
    "shist = []\n",
    "\n",
    "SHlist = [float(h) for h in Scratch]\n",
    "FLlist = [float(h) for h in FinalLayers]\n",
    "FTlist = [float(h) for h in FineTuned]\n",
    "\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "\n",
    "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.plot(range(1,6),SHlist,label=\"Random Initilization\")\n",
    "plt.plot(range(1,6),FLlist,label=\"Transfter Learning Feature Extraction\")\n",
    "plt.plot(range(1,6),FTlist,label=\"Transfter Learning Fine Tuning All Layers\")\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1, 6, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test comparison\n",
    "data = {}\n",
    "data['Random Initilization']=ScratchMetric[1]\n",
    "data['Transfer Learning Feature Extraction']=FinalLayerMetric[1]\n",
    "data['Transfer Learning Fine Tuning']=FineTunedMetric[1]\n",
    "networks = list(data.keys())\n",
    "accuracies = list(data.values())\n",
    "  \n",
    "fig = plt.figure(figsize = (10, 5))\n",
    " \n",
    "# creating the bar plot\n",
    "plt.bar(networks[0], accuracies[0], color ='blue',\n",
    "        width = 0.3)\n",
    "plt.bar(networks[1], accuracies[1], color ='red',\n",
    "        width = 0.3)\n",
    "plt.bar(networks[2], accuracies[2], color ='green',\n",
    "        width = 0.3)\n",
    "plt.ylim((0.6,1.))\n",
    "\n",
    "plt.xlabel(\"ResNet-18 Networks\")\n",
    "plt.ylabel(\"Accuracies\")\n",
    "plt.title(\"Test Set Results\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Knowledge Check</b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz('../quiz_files/submodule_01/kc3.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The results show that using transfer learning and learning all the weights of the network (fine-tuning) achieves the highest accuracy. It shows that we can leverage a pre-trained model to learn features of a new dataset quickly and accurately. \n",
    "Using transfer learning with feature extraction and only learning the last layers is the fastest to train but with less parameters to learn the accuracy of the model on the testing is lower than learning all the weights either from scratch or using pre-trained ImageNet weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "To keep your workspaced organized remember to: \n",
    "\n",
    "1. Save your work.\n",
    "2. Shut down any notebooks and active sessions to avoid extra charges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bedrock (Optional)\n",
    "--------\n",
    "\n",
    "If you're having trouble with this submodule (or others within this tutorial), feel free to leverage Bedrock by running the cell below. Bedrock is a fully managed service that simplifies building and scaling generative AI applications. It provides access to various foundation models (FMs) from Amazon and other AI companies.\n",
    "\n",
    "Before being able to use the chatbot you must request **Llama 3 8B Instruct** model access through AWS Bedrock. In order to do this follow the instructions to request model access provided in [AWS Bedrock Intro Notebook](https://github.com/STRIDES/NIHCloudLabAWS/blob/main/notebooks/GenAI/AWS_Bedrock_Intro.ipynb). After requesting the Llama 3 8B Instruct access it should only take a minute to get approved. While waiting for model approval attach the **AmazonBedrockFullAccess** permission to your notebook service role. Once approved run the following code cell to use the model within the notebook. Keep in mind that you will need to save the util folder with the genai.py file in the same directory as the notebook where you are running the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have the necessary libraries installed\n",
    "!pip install -q ipywidgets\n",
    "import sys\n",
    "import os\n",
    "util_path = os.path.join(os.getcwd(), 'util')\n",
    "if util_path not in sys.path:\n",
    "    sys.path.append(util_path)\n",
    "\n",
    "# Import the display_widgets function from your Python file\n",
    "from genai import display_widgets\n",
    "\n",
    "# Call the function to display the widgets\n",
    "display_widgets()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
